import torch
import torch.nn as nn
import torch.nn.functional as F


class ScaleAdaptiveFusion(nn.Module):
    def __init__(self, dim=256, num_scales=3):
        super().__init__()
        
        # ğŸ”¥ ä¿®æ”¹ 1: å°ºåº¦å¯¹é½ Norm
        # é˜²æ­¢ Layer 0 å› ä¸ºæ•°å€¼å¤§è€Œå¤©ç„¶å ä¼˜ï¼Œå¼ºè¿«å®ƒä»¬åœ¨åŒä¸€èµ·è·‘çº¿ç«äº‰
        self.scale_norms = nn.ModuleList([nn.LayerNorm(dim) for _ in range(num_scales)])
        
        self.score_net = nn.Sequential(
            nn.Linear(dim * 2, dim // 4), 
            nn.ReLU(),
            nn.Dropout(0.1), 
            nn.Linear(dim // 4, 1)
        )
        self.softmax = nn.Softmax(dim=1)
        
        # ğŸ”¥ ä¿®æ”¹ 2: æœ€ç»ˆè¾“å‡ºç¨³å‹ Norm (å¯¹é½ A30/4090 å·®å¼‚çš„å…³é”®)
        self.final_norm = nn.LayerNorm(dim)

    def forward(self, feats_list):
        # 1. å…ˆå¯¹é½å„ä¸ªå°ºåº¦çš„ç‰¹å¾åˆ†å¸ƒ
        normed_feats = [self.scale_norms[i](f) for i, f in enumerate(feats_list)]
        stack = torch.stack(normed_feats, dim=1) # [B, 3, N, C]
        
        B, n_scales, N, C = stack.shape
        
        # 2. è®¡ç®—ç»Ÿè®¡é‡
        avg_p = torch.mean(stack, dim=2)
        max_p = torch.max(stack, dim=2)[0]
        
        # 3. æ‰“åˆ†
        scores = self.score_net(torch.cat([avg_p, max_p], dim=2).view(-1, C*2))
        scores = scores.view(B, n_scales)
        
        # ğŸ”¥ ä¿®æ”¹ 3: é«˜æ¸© Softmax (T=5.0)
        # å¼ºè¡Œæ‹‰å¹³æƒé‡åˆ†å¸ƒï¼Œé˜²æ­¢å‡ºç° [0.99, 0, 0] è¿™ç§æç«¯åˆ†å¸ƒ
        weights_raw = self.softmax(scores / 5.0)
        
        # ğŸ”¥ ä¿®æ”¹ 4: å°ºåº¦ Dropout (ä»…è®­ç»ƒæ—¶)
        # éšæœºæŠŠæŸä¸ªå°ºåº¦çš„æƒé‡æ‰”æ‰ï¼Œé€¼æ¨¡å‹å­¦ä¼šç”¨ Layer 1 å’Œ 2
        if self.training:
            # 10% çš„æ¦‚ç‡ä¸¢å¼ƒæŸä¸ªå°ºåº¦
            scale_mask = (torch.rand(B, n_scales, 1, 1, device=stack.device) > 0.1).float()
            weights_expanded = weights_raw.view(B, n_scales, 1, 1) * scale_mask
            # é‡æ–°å½’ä¸€åŒ–é˜²æ­¢å…¨0
            weights_expanded = weights_expanded / (weights_expanded.sum(dim=1, keepdim=True) + 1e-6)
        else:
            weights_expanded = weights_raw.view(B, n_scales, 1, 1)

        # 4. èåˆ
        fused_feat = torch.sum(stack * weights_expanded, dim=1)
        
        # 5. è¿”å› Norm åçš„ç‰¹å¾
        return self.final_norm(fused_feat), weights_raw

import torch
import torch.nn as nn
import torch.nn.functional as F
'''
# ====================================================================
# 1. åŸºç¡€ Attention æ¨¡å— (æ”¯æŒ FlashAttention + å¤–éƒ¨æ¸©åº¦æ§åˆ¶)
# ====================================================================
class CustomMultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout = dropout
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.proj_dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None, temp_scale=1.0):
        """
        temp_scale: æ¸©åº¦ç¼©æ”¾ç³»æ•°ã€‚
                    > 1.0 : è®©æ³¨æ„åŠ›æ›´å°–é” (é€‚åˆæ·±å±‚)
                    < 1.0 : è®©æ³¨æ„åŠ›æ›´å¹³æ»‘
        """
        B, Lq, _ = query.shape
        B, Lk, _ = key.shape

        # 1. æŠ•å½± + åˆ†å¤´ [B, Heads, Len, HeadDim]
        Q = self.q_proj(query).view(B, Lq, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_proj(key).view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_proj(value).view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)

        # 2. ğŸ”¥ å…³é”®ï¼šåº”ç”¨æ¸©åº¦ç¼©æ”¾
        # F.sdpa é»˜è®¤ç¼©æ”¾æ˜¯ 1/sqrt(d)ã€‚æˆ‘ä»¬ä¹˜ä¸Š temp_scaleï¼Œç­‰æ•ˆäºå…¬å¼ä¸­çš„ Q*K / (sqrt(d) * T)
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªç¼©æ”¾ Q å³å¯
        Q = Q * temp_scale 

        # 3. å¤„ç† Mask (é€‚é… FlashAttention çš„ 4D æ ¼å¼)
        attn_mask = None
        if mask is not None:
            if mask.dim() == 2:
                # [B, Lk] -> [B, 1, 1, Lk] (True=Padding)
                # FlashAttention çš„ mask è¦æ±‚: True è¡¨ç¤ºè¦ mask æ‰çš„ä½ç½® (padding)
                # å¦‚æœä½ çš„ mask æ˜¯ 1=æœ‰æ•ˆ 0=paddingï¼Œé‚£ä¹ˆè¿™é‡Œè¦ç”¨ (mask==0)
                attn_mask = (mask == 0).view(B, 1, 1, Lk).expand(B, self.num_heads, Lq, Lk)
            # å¦‚æœ mask å·²ç»æ˜¯ 4D boolï¼Œç›´æ¥ç”¨

        # 4. è°ƒç”¨ FlashAttention (æé€Ÿã€çœæ˜¾å­˜)
        # is_causal=False (å› ä¸ºè¿™ä¸æ˜¯ GPT ç”Ÿæˆä»»åŠ¡)
        attn_output = F.scaled_dot_product_attention(
            Q, K, V,
            attn_mask=attn_mask,
            dropout_p=self.dropout if self.training else 0.0,
            is_causal=False
        )

        # 5. é‡ç»„è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, Lq, self.embed_dim)
        out = self.out_proj(attn_output)
        out = self.proj_dropout(out)
        
        # æ³¨æ„ï¼šFlashAttention ä¸è¿”å› weightsï¼Œè¿”å› None
        return out, None
'''

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import torch.distributed as dist

def init_weights_safe(m):
    """å¼ºåˆ¶ä½¿ç”¨æå°çš„æ ‡å‡†å·®åˆå§‹åŒ–æŠ•å½±å±‚"""
    if isinstance(m, nn.Linear):
        # 0.005 æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œéå¸¸ä¿å®ˆï¼Œé€‚åˆ Attention æœºåˆ¶
        nn.init.normal_(m.weight, mean=0.0, std=0.005) 
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
# ====================================================================
# 1. åŸºç¡€ Attention æ¨¡å— (æ”¯æŒ FlashAttention + è°ƒè¯•æ¨¡å¼)
# ====================================================================
class CustomMultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.dropout = dropout
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåº”ç”¨å®‰å…¨åˆå§‹åŒ–
        self.q_proj.apply(init_weights_safe)
        self.k_proj.apply(init_weights_safe)
        self.v_proj.apply(init_weights_safe)
        self.out_proj.apply(init_weights_safe)
      
        
        self.proj_dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None, temp_scale=1.0, return_weights=False):
        B, Lq, _ = query.shape
        B, Lk, _ = key.shape
        
       
        # 1. æŠ•å½± + åˆ†å¤´
        Q = self.q_proj(query)
        K = self.k_proj(key)
        V = self.v_proj(value)
        
      
            
        # åˆ†å¤´
        Q = Q.view(B, Lq, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, Lk, self.num_heads, self.head_dim).transpose(1, 2)
        
      
        
        # 2. åº”ç”¨æ¸©åº¦ç¼©æ”¾
        Q = Q * temp_scale
        
      
        attn_weights = None

        # 3. å¤„ç† Mask (FlashAttention æ ¼å¼)
        attn_mask = None
        if mask is not None:
            if mask.dim() == 2:
                # [B, Lk] -> [B, 1, 1, Lk]
                attn_mask = (mask == 0).view(B, 1, 1, Lk).expand(B, self.num_heads, Lq, Lk)

        attn_weights = None

        # ============================================================
        # ğŸ”„ åˆ†æ”¯è·¯å£ï¼šæé€Ÿæ¨¡å¼ vs è°ƒè¯•æ¨¡å¼
        # ============================================================
        if not return_weights:
            # --- æ–¹æ¡ˆ A: æé€Ÿæ¨¡å¼ (FlashAttention) ---
            attn_output = F.scaled_dot_product_attention(
                Q, K, V,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=False
            )
        else:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨torchå†…ç½®çš„ç¨³å®šæ³¨æ„åŠ›
            # è¿™ä¸ªæ–¹æ³•æ¯”æ‰‹åŠ¨è®¡ç®—ç¨³å®šå¾—å¤š
            attn_output, attn_weights = self._stable_attention(
                Q, K, V, attn_mask
            )

        # 5. é‡ç»„è¾“å‡º
        attn_output = attn_output.transpose(1, 2).contiguous().view(B, Lq, self.embed_dim)
        out = self.out_proj(attn_output)
        out = self.proj_dropout(out)
        
        return out, attn_weights
    def _stable_attention(self, Q, K, V, mask):
        
        d_k = self.head_dim
        
        # 1. è®¡ç®—åˆ†æ•° (Softmax æ•°å€¼ç¨³å®šæŠ€å·§å·²ç»å†…ç½®åœ¨ä½ çš„æ‰‹åŠ¨ä»£ç é‡Œäº†)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask, -1e4)
        # ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ä½ çš„æ‰‹åŠ¨ç¨³å®š Softmax é€»è¾‘
        scores_max = torch.max(scores, dim=-1, keepdim=True)[0].detach() 
        scores_stable = scores - scores_max 

        attn_weights = F.softmax(scores_stable, dim=-1)
        
        # Dropout
        p_attn = F.dropout(attn_weights, p=self.dropout, training=self.training)
        
        # åŠ æƒæ±‚å’Œ
        attn_output = torch.matmul(p_attn, V)
        
        return attn_output, attn_weights

# ====================================================================
# 2. å…±äº«ç»„ä»¶åŒ…è£…å™¨ (æ”¯æŒé€ä¼  return_weights)
# ====================================================================
class SharedSelfAttention(nn.Module):
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.attn = CustomMultiHeadAttention(dim, num_heads, dropout)

    def forward(self, x, mask=None, temp=1.0, return_weights=False):
        out, weights = self.attn(
            query=x, key=x, value=x, 
            mask=mask, temp_scale=temp, 
            return_weights=return_weights
        )
        return x + out, weights

class SharedBiModalFusion(nn.Module):
    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.attn_i2t = CustomMultiHeadAttention(dim, num_heads, dropout)
        self.attn_t2i = CustomMultiHeadAttention(dim, num_heads, dropout)
        
        self.injection_gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )

    def forward(self, img, text, text_mask=None, temp=1.0, return_weights=False):
        # 1. å›¾åƒçœ‹æ–‡æœ¬ (I2T)
        i2t_out, weights_i2t = self.attn_i2t(
            img, text, text, mask=text_mask, temp_scale=temp, return_weights=return_weights
        )
        img_enriched = img + i2t_out
        
        # 2. æ–‡æœ¬çœ‹å›¾åƒ (T2I)
        t2i_out, weights_t2i = self.attn_t2i(
            text, img, img, mask=None, temp_scale=temp, return_weights=return_weights
        )
        text_enriched = text + t2i_out
        
        # 3. æ³¨å…¥
        text_context = text_enriched.max(dim=1, keepdim=True)[0]
        text_context_expanded = text_context.expand(-1, img.shape[1], -1)
        
        concat = torch.cat([img_enriched, text_context_expanded], dim=-1)
        gate = self.injection_gate(concat)
        
        output = img_enriched * (1 - gate) + text_context_expanded * gate
        
        return output, (weights_i2t, weights_t2i)

# ====================================================================
# 3. ğŸ”¥ å±‚æ¬¡åŒ–èåˆç³»ç»Ÿ (ä¸»ç±» - å¢åŠ ç‹¬ç«‹æ¸©åº¦å’Œè°ƒè¯•æ¥å£)
# ====================================================================
class AttentionWeightAnalyzer:
    """
    åªåŸºäºæƒé‡åˆ†ææ³¨æ„åŠ›çš„æœ‰æ•ˆæ€§
    """
    
    @staticmethod
    def analyze_weights_simple(weights, temperature, layer_name=""):
        """è¯¦ç»†åˆ†ææ³¨æ„åŠ›æƒé‡"""
        if weights is None:
            print(f"{layer_name}: æƒé‡ä¸ºNone")
            return False, "æ— æƒé‡æ•°æ®"
        
        print(f"\nğŸ” {layer_name} è¯¦ç»†åˆ†æ:")
        print(f"   æƒé‡å½¢çŠ¶: {weights.shape}")
        print(f"   æ¸©åº¦: {temperature}")
        
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€ä¸ªå¤´çš„æƒé‡
        if weights.dim() == 4:  # [B, H, Lq, Lk]
            w = weights[0, 0].detach()
            print(f"   å¤šå¤´æ³¨æ„åŠ›ï¼Œå–å¤´0")
        elif weights.dim() == 3:  # [B, Lq, Lk]
            w = weights[0].detach()
        else:
            print(f"   å¼‚å¸¸ç»´åº¦: {weights.dim()}")
            return False, f"æƒé‡ç»´åº¦å¼‚å¸¸"
        
        Lq, Lk = w.shape
        print(f"   æŸ¥è¯¢é•¿åº¦Lq: {Lq}, é”®é•¿åº¦Lk: {Lk}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaN/Inf
        if torch.isnan(w).any():
            print(f"   âš ï¸ è­¦å‘Š: æƒé‡åŒ…å«NaN!")
        if torch.isinf(w).any():
            print(f"   âš ï¸ è­¦å‘Š: æƒé‡åŒ…å«Inf!")
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦å…¨éƒ¨ç›¸åŒ
        w_flat = w.flatten()
        if (w_flat == w_flat[0]).all():
            print(f"   âš ï¸ è­¦å‘Š: æ‰€æœ‰æƒé‡éƒ½ç›¸åŒ!")
            print(f"   æƒé‡å€¼: {w_flat[0]:.6f}")
            return False, "æ‰€æœ‰æƒé‡ç›¸åŒ"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        print(f"   æƒé‡æœ€å°å€¼: {w.min():.6f}")
        print(f"   æƒé‡æœ€å¤§å€¼: {w.max():.6f}")
        print(f"   æƒé‡å‡å€¼: {w.mean():.6f}")
        print(f"   æƒé‡æ ‡å‡†å·®: {w.std():.6f}")
        
        # æ£€æŸ¥è¡Œå’Œæ˜¯å¦ä¸º1ï¼ˆsoftmaxç‰¹æ€§ï¼‰
        row_sums = w.sum(dim=-1)
        row_sum_error = (row_sums - 1.0).abs().max()
        print(f"   è¡Œå’Œæœ€å¤§è¯¯å·®: {row_sum_error:.6f}")
        
        # è®¡ç®—é›†ä¸­åº¦
        eps = 1e-10
        entropy = -(w * torch.log(w + eps)).sum(dim=-1).mean()
        max_entropy = math.log(Lk)
        concentration = 1 - (entropy / max_entropy).item()
        
        print(f"   ç†µ: {entropy.item():.6f}")
        print(f"   æœ€å¤§ç†µï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰: {max_entropy:.6f}")
        print(f"   é›†ä¸­åº¦: {concentration:.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å‡åŒ€åˆ†å¸ƒ
        uniform_value = 1.0 / Lk
        uniform_diff = (w - uniform_value).abs().mean()
        print(f"   ä¸å‡åŒ€åˆ†å¸ƒçš„å·®å¼‚: {uniform_diff:.6f}")
        
        if concentration < 0.1:
            return False, f"æ³¨æ„åŠ›è¿‡äºåˆ†æ•£(é›†ä¸­åº¦:{concentration:.3f})"
        elif concentration > 0.9:
            return False, f"æ³¨æ„åŠ›è¿‡äºé›†ä¸­(é›†ä¸­åº¦:{concentration:.3f})"
        else:
            return True, f"æ³¨æ„åŠ›æ­£å¸¸(é›†ä¸­åº¦:{concentration:.3f})"
class HierarchicalFusionSystem(nn.Module):
    def __init__(self, text_dim=768, img_dim=256, num_heads=8, dropout=0.1):
        super().__init__()
        
        self.projs = nn.ModuleList([
            nn.Sequential(nn.Linear(text_dim, img_dim), nn.LayerNorm(img_dim), nn.ReLU())
            for _ in range(3)
        ])

        # --- å…±äº«ç»„ä»¶ ---
        self.shared_img_attn = SharedSelfAttention(img_dim, num_heads, dropout)
        self.shared_text_attn = SharedSelfAttention(img_dim, num_heads, dropout)
        self.shared_cross_attn = SharedBiModalFusion(img_dim, num_heads, dropout)

        # --- ç‹¬ç«‹ Norm ---
        self.norms_img_self = nn.ModuleList([nn.LayerNorm(img_dim) for _ in range(3)])
        self.norms_text_self = nn.ModuleList([nn.LayerNorm(img_dim) for _ in range(3)])
        self.norms_img_cross = nn.ModuleList([nn.LayerNorm(img_dim) for _ in range(3)])
        self.norms_text_cross = nn.ModuleList([nn.LayerNorm(img_dim) for _ in range(3)])

        # --- ğŸ”¥ ç‹¬ç«‹æ¸©åº¦å‚æ•° (9ä¸ª) ---
        # 3ä¸ªæ¨¡æ€ç±»å‹ x 3ä¸ªå±‚çº§
        # åˆå§‹åŒ–å€¼å»ºè®®ï¼š1.0 æˆ– 2.0 (æ ¹æ®ä½ ä¹‹å‰çš„å®éªŒï¼Œå¯ä»¥è®¾é«˜ä¸€ç‚¹)
        init_values = torch.tensor([4.0,3.5,3.0]) 



        self.temp_img = nn.Parameter(init_values.clone()) 

        self.temp_txt = nn.Parameter(init_values.clone()) 

        self.temp_cross = nn.Parameter(init_values.clone()) 


        self.fused = ScaleAdaptiveFusion(img_dim, num_scales=3)

        self.sum  = 0
    '''
    def forward(self, text_feats, img_feats, text_mask=None):
        """
        HierarchicalFusionSystem (Single Layer Version)
        """
        fused_outputs = []
        
        # â™»ï¸ åªè¿è¡Œä¸€æ¬¡ (Layer 0) - æˆ–è€…ä½ å¯ä»¥æŒ‡å®šå…·ä½“çš„æŸä¸€å±‚ç´¢å¼•
        # å¦‚æœä½ æƒ³è·‘å•å±‚ï¼Œé€šå¸¸å–æœ€åä¸€å±‚æˆ–è€…ç¬¬ä¸€å±‚ï¼Œè¿™é‡Œç¤ºä¾‹å–ç¬¬ 0 å±‚
        i = 2
        
        # 1. æŠ•å½±å±‚è¾“å‡º
        curr_text = self.projs[i](text_feats[i])
        curr_img = img_feats[i]
        
        # è·å–å½“å‰å±‚çš„æ¸©åº¦ (softplus ä¿è¯ > 0)
        t_img = F.softplus(self.temp_img[i])
        t_txt = F.softplus(self.temp_txt[i])
        t_cross = F.softplus(self.temp_cross[i])
        
        # 2. LayerNorm è¾“å‡º (Attention è¾“å…¥)
        img_in = self.norms_img_self[i](curr_img)
        text_in = self.norms_text_self[i](curr_text)

        # --- æ­¥éª¤ 1 & 2: è‡ªæ³¨æ„åŠ› ---
        curr_img, _ = self.shared_img_attn(
            img_in, mask=None, temp=t_img, return_weights=False
        )
        
        curr_text, _ = self.shared_text_attn(
            text_in, mask=text_mask, temp=t_txt, return_weights=False
        )
        
        # è®¡ç®— Final Representations (ç”¨äº Loss)
        # Global Average Pooling
        final_img_rep = curr_img.mean(dim=1) 
        
        # Masked Mean for Text
        if text_mask is not None:
             mask_broadcast = text_mask.unsqueeze(-1).float()
             final_txt_rep = (curr_text * mask_broadcast).sum(dim=1) / (mask_broadcast.sum(dim=1) + 1e-6)
        else:
             final_txt_rep = curr_text.mean(dim=1)

        # --- æ­¥éª¤ 3: è·¨æ¨¡æ€èåˆ ---
        img_cross_in = self.norms_img_cross[i](curr_img)
        text_cross_in = self.norms_text_cross[i](curr_text)
        
        fused_layer, _ = self.shared_cross_attn(
            img_cross_in, text_cross_in, text_mask, temp=t_cross,
            return_weights=False
        )
        
        #fused_outputs.append(fused_layer)

        # æœ€ç»ˆèåˆ (è™½ç„¶åªæœ‰ä¸€å±‚ï¼Œä½†ä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´ï¼Œè¿˜æ˜¯è¿‡ä¸€ä¸‹ fused æ¨¡å—)
        # å¦‚æœ self.fused æ˜¯å¤„ç†åˆ—è¡¨çš„ï¼Œä¼ å…¥å•å…ƒç´ åˆ—è¡¨å³å¯
        #final_out, _ = self.fused(fused_outputs)

        return fused_layer, final_img_rep, final_txt_rep
    '''
    def forward(self, text_feats, img_feats, text_mask=None):
        """
        HierarchicalFusionSystem çš„ forward æ–¹æ³•
        """
        self.sum = self.sum+1
        return_debug = False
        fused_outputs = []
        sum_c = 1000
        
        # â™»ï¸ å¾ªç¯ 3 æ¬¡ (Layer 0, 1, 2)
        for i in range(3):
        #for i in range(3): 
        #for i in range(2, 3):
        #for i in [2]:
            # 1. æŠ•å½±å±‚è¾“å‡º
            curr_text = self.projs[i](text_feats[i])
            curr_img = img_feats[i]
            
            # è·å–å½“å‰å±‚çš„æ¸©åº¦ (softplus ä¿è¯ > 0)
            t_img = F.softplus(self.temp_img[i])
            t_txt = F.softplus(self.temp_txt[i])
            t_cross = F.softplus(self.temp_cross[i])
            
           

            # 2. LayerNorm è¾“å‡º (Attention è¾“å…¥)
            img_in = self.norms_img_self[i](curr_img)
            text_in = self.norms_text_self[i](curr_text)

           
            
           
            curr_img, w_img = self.shared_img_attn(
                img_in, mask=None, temp=t_img, return_weights=return_debug
            )
            
          
            
            curr_text, w_txt = self.shared_text_attn(
                text_in, mask=text_mask, temp=t_txt, return_weights=return_debug
            )
            
            

            
            # --- æ­¥éª¤ 3: è·¨æ¨¡æ€èåˆ ---
            img_cross_in = self.norms_img_cross[i](curr_img)
            text_cross_in = self.norms_text_cross[i](curr_text)
           
            fused_layer, (w_i2t, w_t2i) = self.shared_cross_attn(
                img_cross_in, text_cross_in, text_mask, temp=t_cross,
                return_weights=return_debug
            )
          
            
                
           
            fused_outputs.append(fused_layer)
            

        # æœ€ç»ˆèåˆ
        final_out, scale_weights = self.fused(fused_outputs)
        
        return final_out, None, None

    