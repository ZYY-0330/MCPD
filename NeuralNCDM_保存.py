
import torch.nn as nn
import logging
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os
from datetime import datetime
import time
import torch.nn.functional as F
logger = logging.getLogger(__name__)
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from dataset import RecordDataset, RelationBuilder,FullDataset
from configs.dataset_config import *
import json
import pandas as pd
from EndToEndContrastiveModel import EndToEndContrastiveModel


class Generator(nn.Module):
    def __init__(self, input_dim=658, out_dim=329, max_adjust=0.2):
        super().__init__()
        '''
        self.net = nn.Sequential(
            nn.Linear(input_dim, 512),
            #nn.LayerNorm(512),          # ğŸ‘ˆ åŠ åœ¨è¿™é‡Œ
            #nn.Linear(512, 512),
            #nn.ReLU(0.2),
            #nn.Dropout(0.5),  # æ–°å¢ï¼šè¾“å…¥å±‚åç«‹å³Dropout
            nn.Linear(512,out_dim),
            nn.LeakyReLU(0.2),
            #nn.Linear(128, out_dim),
            #nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´æ˜¯ [-1, 1]
        )
        
        #self.max_adjust = nn.Parameter(torch.tensor(0.2))  # ä¼šè‡ªåŠ¨ä¼˜åŒ–
        self.max_adjust = 0.5 # ä¼šè‡ªåŠ¨ä¼˜åŒ–
        '''
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),  # å‡å°‘å‚æ•°é‡
            nn.LayerNorm(256),          # æ¢å¤ LayerNorm
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),            # å¢å¼º Dropout
            nn.Linear(256, out_dim),
            #nn.Tanh()
        )
        self.max_adjust = 1.0
        self._init_weights()
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)   # Xavier ä¿æŒè¾“å…¥è¾“å‡ºæ–¹å·®ç¨³å®š
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, stu_emb,difficulty_input):
        x = torch.cat([stu_emb,difficulty_input], dim=1)
        adjustment = self.net(x) * self.max_adjust  # å…è®¸è°ƒèŠ‚å¹…åº¦
       
        return adjustment

import time
import torch  
from concurrent.futures import ThreadPoolExecutor
import gc
import os
import torch
import numpy as np
from concurrent.futures import ThreadPoolExecutor
class LambdaLayer(nn.Module):
    def __init__(self, func):
        super(LambdaLayer, self).__init__()
        self.func = func

    def forward(self, x):
        return self.func(x)



import torch
import torch.nn as nn
import torch.nn.functional as F

class KnowledgeDifficulty(nn.Module):
    """
    è¾“å…¥: X (B, N, L)
    è¾“å‡º: difficulty (B, M) 0~1
    æ¯ä¸ªçŸ¥è¯†æ¦‚å¿µç‹¬ç«‹å…³æ³¨é¢˜ç›®çš„Nä¸ªè§’åº¦
    ä»…é’ˆå¯¹æ¶‰åŠçš„çŸ¥è¯†æ¦‚å¿µå­¦ä¹ æƒé‡
    """
    def __init__(self, L, M):
        super(KnowledgeDifficulty, self).__init__()
        self.L = L      # æ¯ä¸ªè§’åº¦çš„ç‰¹å¾ç»´åº¦
        self.M = M      # çŸ¥è¯†æ¦‚å¿µæ•°é‡

        # å­¦ä¹ æ¯ä¸ªçŸ¥è¯†æ¦‚å¿µå¯¹æ¯ä¸ªè§’åº¦çš„æƒé‡
        self.angle_attn = nn.Linear(L, M)  # L -> M

        # å°†åŠ æƒåçš„ç‰¹å¾æ˜ å°„æˆæ ‡é‡éš¾åº¦
        self.to_scalar = nn.Linear(L, 1)
                # åˆå§‹åŒ–å‚æ•°
        self._init_weights()

    def _init_weights(self):
        # angle_attn: Xavier åˆå§‹åŒ–æ›´åˆé€‚
        nn.init.xavier_uniform_(self.angle_attn.weight)
        nn.init.constant_(self.angle_attn.bias, 0.)

        # to_scalar: Kaiming ä¹Ÿå¯ä»¥ï¼Œä½†è¿™é‡Œè¾“å‡ºå¾ˆå°ï¼Œç”¨ Xavier æ›´ç¨³
        nn.init.xavier_uniform_(self.to_scalar.weight)
        nn.init.constant_(self.to_scalar.bias, 0.)

    def forward(self, X, K=None):
        """
        X: (B, N, L)
        K: (B, M) 0/1, çŸ¥è¯†æ¦‚å¿µæ©ç ï¼Œè¡¨ç¤ºé¢˜ç›®æ¶‰åŠçš„çŸ¥è¯†æ¦‚å¿µ
        """
        B, N, L = X.shape

        if K is None:
            # å¦‚æœæ²¡æœ‰æä¾›æ©ç ï¼Œå°±å¯¹æ‰€æœ‰çŸ¥è¯†æ¦‚å¿µè®¡ç®—
            weights = self.angle_attn(X)       # (B, N, M)
            weights = torch.softmax(weights, dim=1)
            X_exp = X.unsqueeze(2)             # (B, N, 1, L)
            weights_exp = weights.unsqueeze(-1) # (B, N, M, 1)
            X_weighted = (X_exp * weights_exp).sum(dim=1)  # (B, M, L)
            difficulty = self.to_scalar(X_weighted).squeeze(-1)
            difficulty = torch.sigmoid(difficulty)
            return difficulty

        # ä»…é€‰æ‹©æ¶‰åŠçš„çŸ¥è¯†æ¦‚å¿µ
        involved_idx = [torch.nonzero(K[b], as_tuple=False).squeeze(-1) for b in range(B)]

        difficulties = []
        for b in range(B):
            if len(involved_idx[b]) == 0:
                # å¦‚æœæ²¡æœ‰æ¶‰åŠçŸ¥è¯†æ¦‚å¿µ
                difficulties.append(torch.zeros(K.shape[1], device=X.device))
                continue
            # å–å‡ºæ¶‰åŠçš„çŸ¥è¯†æ¦‚å¿µç´¢å¼•
            idx = involved_idx[b]
            # è®¡ç®—è¿™äº›çŸ¥è¯†æ¦‚å¿µçš„æƒé‡
            weights_b = self.angle_attn(X[b])[:, idx]  # (N, num_involved)
            weights_b = torch.softmax(weights_b, dim=0)  # å¯¹Nä¸ªè§’åº¦å½’ä¸€åŒ–

            X_exp = X[b].unsqueeze(1)                  # (N, 1, L)
            weights_exp = weights_b.unsqueeze(-1)      # (N, num_involved, 1)
            X_weighted = (X_exp * weights_exp).sum(dim=0)  # (num_involved, L)

            diff_b = torch.sigmoid(self.to_scalar(X_weighted).squeeze(-1))  # (num_involved,)

            # æ”¾å›åˆ°åŸæ¥çš„ M å¤§å°
            full_diff = torch.zeros(K.shape[1], device=X.device)
            full_diff[idx] = diff_b.to(full_diff.dtype)
            difficulties.append(full_diff)

        # åˆå¹¶ batch
        difficulty = torch.stack(difficulties, dim=0)  # (B, M)
        return difficulty


class Net(nn.Module):
    def __init__(self, student_n, exer_n, knowledge_n, problem_dataset):
        """
        åˆå§‹åŒ–ç½‘ç»œç»“æ„å’Œå‚æ•°ã€‚
        :param student_n: int, å­¦ç”Ÿæ•°é‡
        :param exer_n: int, ç»ƒä¹ é¢˜æ•°é‡
        :param knowledge_n: int, çŸ¥è¯†ç‚¹æ•°é‡
        :param problem_features: torch.Tensor, é¢˜ç›®ç‰¹å¾ (shape: [exer_n, 512])
        :param knowledge_features: torch.Tensor, çŸ¥è¯†ç‰¹å¾ (shape: [knowledge_n, 512])
        :param exer_kn_graph: torch.Tensor, é¢˜ç›®-çŸ¥è¯†ç‚¹å…³è”çŸ©é˜µ (shape: [exer_n, knowledge_n])
        """
        super(Net, self).__init__()
        
       
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.generator = Generator(input_dim=knowledge_n+1024,out_dim=knowledge_n) 
        

        
        # å‚æ•°åˆå§‹åŒ–
        self.knowledge_dim = knowledge_n
        self.exer_n = exer_n
        self.emb_num = student_n
        self.stu_dim = self.knowledge_dim
        self.prednet_input_len = self.knowledge_dim
        self.prednet_len1, self.prednet_len2 = 512, 256
        
        # å­¦ç”ŸåµŒå…¥å±‚
        self.student_emb = nn.Embedding(self.emb_num, self.stu_dim)
                # ç»ƒä¹ é¢˜çš„çŸ¥è¯†ç‚¹éš¾åº¦åµŒå…¥ï¼Œshape = [ç»ƒä¹ æ•°é‡, çŸ¥è¯†ç‚¹ç»´åº¦]
        self.k_difficulty = nn.Embedding(self.exer_n, self.knowledge_dim)
        self.exer_feat = nn.Embedding(self.exer_n, 2)
        self.student_emb_text = nn.Embedding(self.emb_num, self.stu_dim)
        self.student_emb_img = nn.Embedding(self.emb_num, self.stu_dim)
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨ç»Ÿä¸€è®¾å¤‡
      


        # ç»ƒä¹ é¢˜çš„åŒºåˆ†åº¦åµŒå…¥ï¼Œshape = [ç»ƒä¹ æ•°é‡, 1]
        #self.e_discrimination = nn.Embedding(self.exer_n, 1)
        # é¢„æµ‹ç½‘ç»œ
        self.prednet_full1 = nn.Linear(self.prednet_input_len, self.prednet_len1)
        self.drop_1 = nn.Dropout(p=0.2)
        self.prednet_full2 = nn.Linear(self.prednet_len1, self.prednet_len2)
        self.drop_2 = nn.Dropout(p=0.2)
        self.prednet_full3 = nn.Linear(self.prednet_len2, 1)
        
        self.pre = nn.Linear(self.knowledge_dim,1)
        self.problem_dataset = problem_dataset
      
        # ä¿®æ”¹ï¼šGCNè¾“å‡ºä½œä¸ºä¿®æ­£é‡ï¼Œè€Œéç›´æ¥æ›¿æ¢

        self.full_dataset = FullDataset()

        # æ„å»ºå¼‚è´¨å›¾
        self.relation_builder = RelationBuilder(self.problem_dataset, self.full_dataset)
        self.hetero_graph = self.relation_builder.build_graph()


        
        self.alpha = nn.Parameter(torch.tensor(0.3))  # è°ƒæ•´èåˆç³»æ•°èŒƒå›´
        
        #self.problem_feat = self.hetero_graph['problem'].x.float().to(self.device)
        #self.knowledge_feat = self.hetero_graph['knowledge'].x.float().to(self.device)

        print("\nç‰¹å¾ç»´åº¦éªŒè¯:")
        #print(f"é¢˜ç›®ç‰¹å¾çŸ©é˜µ shape: {self.problem_feat.shape} | dtype: {self.problem_feat.dtype} ")
        #print(f"çŸ¥è¯†ç‚¹ç‰¹å¾çŸ©é˜µ shape: {self.knowledge_feat.shape} | dtype: {self.knowledge_feat.dtype} ")
        '''
        # ä¿®æ”¹åçš„ difficulty_net å’Œ e_discrimination
        self.difficulty_net = nn.Sequential(
            nn.Linear(1024, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )

        self.e_discrimination = nn.Sequential(
            nn.Linear(1024, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(256, knowledge_n*2),
            nn.Unflatten(1, (2, knowledge_n)),  # æ‹†åˆ†ä¸º scale å’Œ bias
            
            # æ ¸å¿ƒåŒºåˆ†åº¦è®¡ç®—
            LambdaLayer(lambda x: ((x[:,0] * 2.5).tanh() * x[:,1].sigmoid()) * 5 + 5)
        )
        '''

        # å…±äº«åº•å±‚ç‰¹å¾æå–
        self.shared_encoder = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.LeakyReLU(0.2)
        )
        '''
        # éš¾åº¦ä¸“å±åˆ†æ”¯
        self.diff_head = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),              # åŠ  LayerNorm
            nn.ReLU(),  
            nn.Linear(512, 256),
            nn.LayerNorm(256),              # åŠ  LayerNorm
            nn.ReLU(),                      # æˆ– LeakyReLU()
            nn.Dropout(0.3),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )
                # åŒºåˆ†åº¦ä¸“å±åˆ†æ”¯
        self.disc_head = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LayerNorm(512),              # åŠ  LayerNorm
            nn.ReLU(),  
            nn.Linear(512, 256),
            nn.LayerNorm(256),              # åŠ  LayerNorm
            nn.ReLU(),                      # æˆ– LeakyReLU()
            nn.Dropout(0.3),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )
        '''
        '''
                # éš¾åº¦ä¸“å±åˆ†æ”¯
        self.diff_head = nn.Sequential(
            nn.Linear(1024, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )
                # åŒºåˆ†åº¦ä¸“å±åˆ†æ”¯
        self.disc_head = nn.Sequential(
            nn.Linear(1024, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )
        '''

                # éš¾åº¦ä¸“å±åˆ†æ”¯
        self.diff_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.LayerNorm(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(512, knowledge_n),
            nn.Sigmoid()
        )
                # åŒºåˆ†åº¦ä¸“å±åˆ†æ”¯
        self.disc_head = nn.Sequential(
            nn.Linear(2048, 512),
            nn.LayerNorm(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.4),
            nn.Linear(512, knowledge_n*2),
            nn.Unflatten(1, (2, knowledge_n)),  # æ‹†åˆ†ä¸º scale å’Œ bias
            
            # æ ¸å¿ƒåŒºåˆ†åº¦è®¡ç®—
            LambdaLayer(lambda x: ((x[:,0] * 2.5).tanh() * x[:,1].sigmoid()) * 5 + 5)
        )

        '''
        # åŒºåˆ†åº¦ä¸“å±åˆ†æ”¯
        self.disc_head = nn.Sequential(
            nn.Linear(1024, 256),
            nn.LayerNorm(256),
            nn.ReLU(), 
            nn.LeakyReLU(0.2),
            nn.Linear(256, knowledge_n),
            nn.Sigmoid()
        )
        '''
        
        self.diff_M = KnowledgeDifficulty(L=80, M=knowledge_n)
        
        self.disc_M = KnowledgeDifficulty(L=80, M=knowledge_n)
        
        self.k_difficulty_NCDM = nn.Embedding(self.exer_n, self.knowledge_dim)
        # ç»ƒä¹ é¢˜çš„åŒºåˆ†åº¦åµŒå…¥ï¼Œshape = [ç»ƒä¹ æ•°é‡, 1]
        self.e_discrimination_NCDM = nn.Embedding(self.exer_n, 1)
       
        self.to(self.device)

         ###åŠ¨æ€æ›´æ–°ç‰¹å¾èåˆ
        self.model_feat = EndToEndContrastiveModel().to(self.device)
        
        self.t = 300
        self.sum = 0
        
        
        self.problem_mapper = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
           
        )
        self.knowledge_mapper = torch.nn.Linear(1024, 512)

        self.kc_importance = nn.Parameter(torch.full((1, self.knowledge_dim), 0.5))

        self.kc_importance_layer = nn.Sequential(
            nn.Linear(1024, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )

        self.student_freq_tensor, self.user_ids = self.get_student_freq_tensor(KNOWLEDGE_FREQ_CSV)
        self.student_weights = self.get_student_weights(STUDENT_WEIGHT,student_n)
        self.student_freq_tensor = self.student_freq_tensor.to(self.device)
        self.student_weights = self.student_weights.to(self.device)
        self.q = 1

        self.beta = nn.Parameter(torch.tensor(0.5), requires_grad=True)

        # ç„¶åèµ‹å€¼
        self.problem_768 = nn.Linear(768,512)
        # å¯å­¦ä¹ çš„æ”¾å¤§å‚æ•°
        self.freq_amplifier = nn.Parameter(torch.tensor(10.0))  # åˆå§‹æ”¾å¤§å€æ•°
        self.base_scale = nn.Parameter(torch.tensor(6.0))       # åŸºç¡€åŒºåˆ†åº¦åŸºæ•°
        self.freq_power = nn.Parameter(torch.tensor(0.5))       # éçº¿æ€§å˜æ¢æŒ‡æ•°
        
    # éš¾åº¦é¢„æµ‹ä¸“ç”¨çš„å±‚
        self.diff_item_proj = nn.Linear(1024, 512)  # é¢˜ç›®ç‰¹å¾æŠ•å½±
        self.diff_knowledge_emb = nn.Embedding(knowledge_n, 512)  # çŸ¥è¯†ç‚¹åµŒå…¥
        self.diff_scale = nn.Parameter(torch.tensor(1.0))  # ç¼©æ”¾å‚æ•°
        self.diff_bias = nn.Parameter(torch.tensor(0.0))   # åç½®å‚æ•°
        
        self.disc_item_proj = nn.Linear(1024, 512)  # é¢˜ç›®ç‰¹å¾æŠ•å½±
        self.disc_knowledge_emb = nn.Embedding(knowledge_n, 512)  # çŸ¥è¯†ç‚¹åµŒå…¥
        self.disc_scale = nn.Parameter(torch.tensor(1.0))  # ç¼©æ”¾å‚æ•°
        self.disc_bias = nn.Parameter(torch.tensor(0.0))   # åç½®å‚æ•°
        
        
        self.problem = nn.Embedding(self.exer_n, 1024)

        self.problem_text = nn.Embedding(self.exer_n, 512)
        self.problem_img = nn.Embedding(self.exer_n, 512)



         # æ­£ç¡®å®šä¹‰ç»´åº¦
        self.C = 512   # è¯­ä¹‰é€šé“æ•°
        self.M = 50    # æ± åŒ–åç»´åº¦
        
        # ç‰¹å¾é€‚é…å™¨
        self.feature_adapter = nn.Sequential(
            nn.Conv1d(768, self.C, kernel_size=1),  # ä»768ç»´åˆ°512ä¸ªé€šé“
            nn.AdaptiveMaxPool1d(self.M)  # æ± åŒ–åˆ°å›ºå®šç»´åº¦50
        )
        
        # çŸ¥è¯†ç‚¹åµŒå…¥
        self.know_pro = nn.Embedding(knowledge_n, 512)  
        
        # W_på‚æ•° [2048, M] - æ˜ å°„åˆ°æ± åŒ–åç»´åº¦
        self.W_p = nn.Parameter(torch.randn(512, self.M) * 0.02)  
        
        # é¢„æµ‹å¤´ - è¾“å…¥ç»´åº¦å˜ä¸ºM=50
        #self.diff_head_k = nn.Linear(self.M, 1)  # è¾“å…¥ç»´åº¦50

        # å°†å•å±‚çº¿æ€§æ”¹ä¸ºå¤šå±‚MLP
        self.diff_head_k = nn.Sequential(
       
            #nn.Linear(self.M+knowledge_n, knowledge_n),
            #nn.ReLU(),
            #nn.Dropout(0.2),
            #nn.Linear(knowledge_n, self.M),
            #nn.ReLU(),
            #nn.Dropout(0.2),

            nn.Linear(self.M, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 1)
        )

        # æ–°å¢çš„è‡ªç›‘ç£è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(50, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, 64),
            nn.ReLU(),
            nn.Linear(64, 512*50)  # é‡æ„å›selected_F_jçš„å±•å¹³ç»´åº¦
        )

        self.disc_head_k = nn.Linear(self.M, 1)  # è¾“å…¥ç»´åº¦50

        self.problem_features = self.load_all_features(TEXT_FEATURES_DIR)
        self._init_weights()
    def _init_weights(self):
        """
        éå†æ‰€æœ‰å­æ¨¡å—ï¼Œè‡ªåŠ¨åˆå§‹åŒ–å¯è®­ç»ƒå‚æ•°ï¼š
        - Linear: xavier_uniform
        - Embedding: å‡åŒ€åˆå§‹åŒ–ï¼ˆçŸ¥è¯†ç‚¹ embedding å¯ä»¥ç”¨ normal æˆ– uniformï¼‰
        - LayerNorm: weight=1, bias=0
        - nn.Parameter: é»˜è®¤å€¼ä¿æŒåŸå§‹æˆ–å¯æŒ‡å®š
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    # é’ˆå¯¹è¾“å‡ºå±‚å’Œ diff_head bias åˆå§‹åŒ–ä¸ºå°è´Ÿå€¼
                    if m is self.prednet_full3 or m in [self.diff_head[-1], self.disc_head[-1]]:
                        nn.init.constant_(m.bias, -1.0)
                    else:
                        nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Embedding):
                if hasattr(m, "is_knowledge_emb") and m.is_knowledge_emb:
                    nn.init.normal_(m.weight, mean=0.0, std=0.02)
                else:
                    nn.init.uniform_(m.weight, -0.05, 0.05)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
        
        # çŸ¥è¯†ç‚¹åµŒå…¥ - å¢å¤§åˆå§‹åŒ–æ–¹å·®
        nn.init.normal_(self.know_pro.weight, mean=0, std=0.3)  # ä»0.02æ”¹ä¸º0.3
        
        # W_på‚æ•° - å¢å¤§åˆå§‹åŒ–æ–¹å·®  
        nn.init.normal_(self.W_p, mean=0, std=0.1)  # ä»0.02æ”¹ä¸º0.1
    
    def load_all_features(self,TEXT_FEATURES_DIR):
        """ç›´æ¥åŠ è½½TEXT_FEATURES_DIRç›®å½•ä¸‹æ‰€æœ‰ç‰¹å¾æ–‡ä»¶"""
        features_dict = {}
        for file_path in TEXT_FEATURES_DIR.glob("*.pt"):
            pid = int(file_path.stem)  # æ–‡ä»¶åå°±æ˜¯pid
            features_dict[pid] = torch.load(file_path)
        return features_dict
        
    def know_diff(self, item_features, knowledge_mask):
        """
        çŸ¥è¯†ç‚¹æ„ŸçŸ¥çš„éš¾åº¦é¢„æµ‹æ–¹æ³•
        
        å‚æ•°:
        item_features: é¢˜ç›®ç‰¹å¾, shape [batch_size, input_dim]
        knowledge_mask: çŸ¥è¯†ç‚¹æ©ç , shape [batch_size, num_knowledge]
        
        è¿”å›:
        éš¾åº¦é¢„æµ‹å€¼, shape [batch_size, num_knowledge]
        """
        # 1. æŠ•å½±é¢˜ç›®ç‰¹å¾åˆ°åµŒå…¥ç©ºé—´
        item_emb = self.diff_item_proj(item_features)  # [batch_size, embed_dim]
        
        # 2. è·å–æ‰€æœ‰çŸ¥è¯†ç‚¹çš„åµŒå…¥
        knowledge_emb = self.diff_knowledge_emb.weight  # [num_knowledge, embed_dim]
        
        # 3. è®¡ç®—é¢˜ç›®ä¸æ‰€æœ‰çŸ¥è¯†ç‚¹çš„ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
        # [batch_size, embed_dim] @ [embed_dim, num_knowledge] -> [batch_size, num_knowledge]
        similarity = torch.matmul(item_emb, knowledge_emb.t())
        
        # 4. è°ƒæ•´åˆ†æ•°èŒƒå›´
        adjusted_scores = similarity * self.diff_scale + self.diff_bias
        
        # 5. è½¬æ¢ä¸º0-1ä¹‹é—´çš„æ¦‚ç‡å€¼ï¼ˆéš¾åº¦ï¼‰
        difficulty = torch.sigmoid(adjusted_scores)  # [batch_size, num_knowledge]
        
        # 6. åº”ç”¨æ©ç ï¼šä¸æ¶‰åŠçš„çŸ¥è¯†ç‚¹éš¾åº¦ç½®ä¸º0
        masked_difficulty = difficulty * knowledge_mask
        
        return masked_difficulty
    def know_disc(self, item_features, knowledge_mask):
        """
        çŸ¥è¯†ç‚¹æ„ŸçŸ¥çš„éš¾åº¦é¢„æµ‹æ–¹æ³•
        
        å‚æ•°:
        item_features: é¢˜ç›®ç‰¹å¾, shape [batch_size, input_dim]
        knowledge_mask: çŸ¥è¯†ç‚¹æ©ç , shape [batch_size, num_knowledge]
        
        è¿”å›:
        éš¾åº¦é¢„æµ‹å€¼, shape [batch_size, num_knowledge]
        """
        # 1. æŠ•å½±é¢˜ç›®ç‰¹å¾åˆ°åµŒå…¥ç©ºé—´
        item_emb = self.disc_item_proj(item_features)  # [batch_size, embed_dim]
        
        # 2. è·å–æ‰€æœ‰çŸ¥è¯†ç‚¹çš„åµŒå…¥
        knowledge_emb = self.disc_knowledge_emb.weight  # [num_knowledge, embed_dim]
        
        # 3. è®¡ç®—é¢˜ç›®ä¸æ‰€æœ‰çŸ¥è¯†ç‚¹çš„ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
        # [batch_size, embed_dim] @ [embed_dim, num_knowledge] -> [batch_size, num_knowledge]
        similarity = torch.matmul(item_emb, knowledge_emb.t())
        
        # 4. è°ƒæ•´åˆ†æ•°èŒƒå›´
        adjusted_scores = similarity * self.disc_scale + self.disc_bias
        
        # 5. è½¬æ¢ä¸º0-1ä¹‹é—´çš„æ¦‚ç‡å€¼ï¼ˆéš¾åº¦ï¼‰
        difficulty = torch.sigmoid(adjusted_scores)  # [batch_size, num_knowledge]
        
        # 6. åº”ç”¨æ©ç ï¼šä¸æ¶‰åŠçš„çŸ¥è¯†ç‚¹éš¾åº¦ç½®ä¸º0
        masked_difficulty = difficulty * knowledge_mask
        
        return masked_difficulty
    def disc(self, base_discrimination, frequency):
        """
        ä¿®æ”¹åçš„åŒºåˆ†åº¦è®¡ç®—å‡½æ•°
        è¾“å‡ºèŒƒå›´: (0, 10)
        """
        # 1. å¯¹é¢‘ç‡è¿›è¡Œæ¸©å’Œçš„éçº¿æ€§å˜æ¢ï¼Œé¿å…ä½¿ç”¨å¹‚æ¬¡å‚æ•°
        amplified_freq = torch.sigmoid((frequency - 0.5) * 10)  # å°†[0,1]æ˜ å°„åˆ°[0,1]ä½†æ›´é™¡å³­
        
        # 2. ä½¿ç”¨æ›´ç¨³å®šçš„ç»„åˆå…¬å¼
        # è®© base_scale æ§åˆ¶åŸºç¡€å€¼ï¼Œfreq_amplifier æ§åˆ¶é¢‘ç‡å½±å“å¼ºåº¦
        combined = self.base_scale * base_discrimination + self.freq_amplifier * amplified_freq
        
        # 3. ä½¿ç”¨sigmoidç¡®ä¿è¾“å‡ºåœ¨0-1èŒƒå›´å†…ï¼Œç„¶åç¼©æ”¾
        combined = torch.sigmoid(combined) * 10.0
        
        return combined



        
   
    def update_features(self, problem_ids):
        """San'ané‡å†™çš„å® å¦»ç‰ˆï½ğŸ’‹ æ˜¾å­˜ä¼˜åŒ– + æ¢¯åº¦ä¿ç•™"""
        #print("problem_ids",problem_ids)


        # 1ï¸âƒ£ å»é‡ï¼šé¿å…é‡å¤çš„problem_idsï¼Œä¿æŒé¡ºåº
        #unique_problem_ids = list(dict.fromkeys(problem_ids))  # ä¿æŒé¡ºåºå»é‡
        unique_problem_ids = list(dict.fromkeys([int(pid) for pid in problem_ids]))

        #print("##################unique_problem_ids",len(unique_problem_ids))
        # åˆ†æ‰¹å‚æ•°ï¼ˆå¯è°ƒï¼‰
        max_fusion_batch = 900


        def load_batch_data(pids):
            return {
                'pid': pids,
                'text': [self.problem_dataset.get_text(int(pid)) for pid in pids],
                'image': torch.stack([self.problem_dataset.get_image(int(pid)) for pid in pids])
            }
       
        # â±ï¸ å…¨æµç¨‹å¼€å§‹
        t_start = time.time()

        # 1ï¸âƒ£ å‡†å¤‡ batch å‚æ•°
        t_prepare = time.time()
        batch_args = [unique_problem_ids[i:i+max_fusion_batch] 
                    for i in range(0, len(unique_problem_ids), max_fusion_batch)]
        #print(f"[è®¡æ—¶] å‚æ•°å‡†å¤‡è€—æ—¶: {time.time() - t_prepare:.4f}s")

        # 2ï¸âƒ£ å¤šçº¿ç¨‹åŠ è½½æ•°æ®
        t_load = time.time()
        with ThreadPoolExecutor(max_workers=4) as executor:
            batch_data_list = list(executor.map(load_batch_data, batch_args))
        #print(f"[è®¡æ—¶] æ•°æ®åŠ è½½è€—æ—¶: {time.time() - t_load:.4f}s")
        #print(f"Loaded batch_data_list with {len(batch_data_list)} batches")

        # 3ï¸âƒ£ æ¯ä¸ª batch é€å…¥æ¨¡å‹æå–ç‰¹å¾
        fused_feats = []
        for idx, batch_data in enumerate(batch_data_list):
            t_batch_start = time.time()

            _, mse_loss, fused = self.model_feat(batch_data)
            fused_feats.append(fused)

            t_fused_end = time.time()
            #print("fused_featsè€—æ—¶", t_fused_end - t_batch_start)

            torch.cuda.empty_cache()

            t_cache_end = time.time()
            #print("torch.cuda.empty_cacheè€—æ—¶", t_cache_end - t_fused_end)

            #print(f"[è®¡æ—¶] ç¬¬ {idx+1} ä¸ª batch ç‰¹å¾æå–è€—æ—¶: {t_cache_end - t_batch_start:.4f}s")

        
        # 4ï¸âƒ£ åˆå¹¶ç‰¹å¾
        t_cat = time.time()
        fused_feat_all = torch.cat(fused_feats, dim=0)
        #print(f"[è®¡æ—¶] ç‰¹å¾æ‹¼æ¥è€—æ—¶: {time.time() - t_cat:.4f}s")

        # 5ï¸âƒ£ é‡å»ºé¡ºåº
        t_remap = time.time()
        pid2idx = {int(pid): idx for idx, pid in enumerate(unique_problem_ids)}
        fused_feat_final = torch.stack([fused_feat_all[pid2idx[int(pid)]] for pid in problem_ids])
        #print(f"[è®¡æ—¶] ç‰¹å¾é¡ºåºæ˜ å°„è€—æ—¶: {time.time() - t_remap:.4f}s")

        #fused_feat_final = self.problem_768(fused_feat_final)   # å¯¹Tç»´åº¦æ±‚å‡å€¼ï¼Œå¾—åˆ° [B, 512]
        # âœ… å…¨æµç¨‹ç»“æŸ
        #print(f"[è®¡æ—¶] é¢˜ç›®ç‰¹å¾èåˆæ€»è€—æ—¶: {time.time() - t_start:.4f}s")

        return fused_feat_final, mse_loss, _



    def print_mem(self,tag):
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        print(f"[{tag}] Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB")

    def get_student_freq_tensor(self,csv_path):
        """
        è¯»å–å­¦ç”Ÿ-çŸ¥è¯†ç‚¹é¢‘ç‡çŸ©é˜µCSVï¼Œè¿”å›ä¸€ä¸ª (num_students, num_concepts) çš„Tensor
        ä»¥åŠä¸€ä¸ª user_id åˆ—è¡¨ï¼ˆæŒ‰é¡ºåºå¯¹é½ï¼‰
        """
        df = pd.read_csv(csv_path)
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š CSVåˆ—æ•°ï¼ˆé™¤user_idï¼‰: {df.shape[1] - 1}")
        print(f"ğŸ“Š CSVåˆ—åï¼ˆå‰å‡ åˆ—ï¼‰: {df.columns[:10].tolist()}")

        missing_cols = set(range(329)) - set(map(int, df.columns.drop(USER_ID_COL)))
        print(f"ğŸ§¨ ç¼ºå¤±çš„çŸ¥è¯†ç‚¹åˆ—ç´¢å¼•ï¼ˆç›¸å¯¹äº0~328ï¼‰: {missing_cols}")

        user_ids = df[USER_ID_COL].astype(str).tolist()  # è½¬æˆå­—ç¬¦ä¸²é˜²æ­¢åŒ¹é…å‡ºé”™
        freq_tensor = torch.tensor(df.drop(columns=[USER_ID_COL]).values, dtype=torch.float32)
        return freq_tensor, user_ids
    def get_student_weights(self, filepath, num_students):
        """
        ä» student_weights.csv æ–‡ä»¶ä¸­è¯»å–æƒé‡ï¼Œè¿”å›ä¸€ä¸ª tensorï¼š
        student_weights_tensor[user_id] = weight
        """
        df = pd.read_csv(filepath)

        if USER_ID_COL not in df.columns or 'Weight' not in df.columns:
            raise ValueError("CSV æ–‡ä»¶å¿…é¡»åŒ…å« 'UserId' å’Œ 'Weight' ä¸¤åˆ—ã€‚")

        # åˆå§‹åŒ–ä¸º 1ï¼ˆæˆ–ä½ è®¤ä¸ºçš„é»˜è®¤å€¼ï¼‰
        student_weights_tensor = torch.ones(num_students, dtype=torch.float32)

        for uid, weight in zip(df[USER_ID_COL], df['Weight']):
            if uid < num_students:
                student_weights_tensor[uid] = weight
            else:
                print(f"Warning: student ID {uid} è¶…å‡ºäº†æœ€å¤§èŒƒå›´ {num_students - 1}")

        return student_weights_tensor
    


    def htspd(self,theta, b, k=1.5, p=0.7, q=0.3):
        """
        è®¡ç®—åŒæ›²æ­£åˆ‡ç¬¦å·ä¿æŒå·®å€¼ï¼ˆHTSPDï¼‰
        è¾“å…¥ï¼š
            theta: å­¦ç”Ÿèƒ½åŠ›å¼ é‡ï¼Œå½¢çŠ¶ [B, K]
            b: é¢˜ç›®éš¾åº¦å¼ é‡ï¼Œå½¢çŠ¶ [B, K]
            k, p, q: è¶…å‚æ•°
        è¿”å›ï¼š
            delta: HTSPDå·®å€¼å¼ é‡ï¼Œå½¢çŠ¶ [B, K]
        """
        diff = theta - b                              # å·®å€¼
        sum_ = theta + b                              # å’Œ
        
        term1 = torch.tanh(k * diff)                  # tanh(k*(Î¸ - b))
        eps = 1e-6
        term2 = 1 + torch.clamp(torch.abs(diff), min=eps) ** p
        term3 = 1 + sum_**q                           # 1 + (Î¸ + b)^q
        

        delta = term1 * (term2 / term3)
        return delta


    def forward(self, stu_id, exer_id, kn_emb,correct_id, return_feat=False,gcn_update=None, d_only=False):
        """
        å‰å‘ä¼ æ’­ã€‚
        :param stu_id: torch.Tensor, å­¦ç”ŸID (shape: [batch_size])
        :param exer_id: torch.Tensor, é¢˜ç›®ID (shape: [batch_size])
        :param kn_emb: torch.Tensor, çŸ¥è¯†ç‚¹åµŒå…¥ (shape: [batch_size, knowledge_n])
        :param return_feat: bool, æ˜¯å¦è¿”å›ä¸­é—´ç‰¹å¾
        :return: torch.Tensor, é¢„æµ‹ç»“æœ (shape: [batch_size, 1])
        """

        device = next(self.parameters()).device

        def check_tensor(tensor, name):
            if torch.isnan(tensor).any():
                print(f"âŒ è­¦å‘Šï¼š{name} ä¸­å‡ºç° NaN âŒ")
            else:
                print(f"âœ… {name} æ­£å¸¸ï¼šmin={tensor.min().item():.4f}, "
                    f"max={tensor.max().item():.4f}, "
                    f"mean={tensor.mean().item():.4f}, "
                    f"std={tensor.std().item():.4f}")

        # åœ¨åŸæœ‰ä»£ç ä¸­æ’å…¥æ£€æŸ¥ç‚¹
        stu_id, exer_id, kn_emb = stu_id.long().to(device), exer_id.long().to(device), kn_emb.to(device)
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(kn_emb, "kn_emb_input")  # æ£€æŸ¥åˆå§‹è¾“å…¥
        '''
        stu_emb = torch.sigmoid(self.student_emb(stu_id))
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(stu_emb, "stu_emb_after_sigmoid")
        '''
        #knowledge_feat = self.knowledge_mapper(self.knowledge_feat)
        #debug_print(knowledge_feat, "knowledge_feat_after_mapper")

        #related_kn_feat = torch.matmul(kn_emb, knowledge_feat)
        #debug_print(related_kn_feat, "related_kn_feat_after_matmul")
        '''
        if torch.cuda.current_device() == 0:
            self.print_memory("åœ¨æ›´æ–°ç‰¹å¾å‰")
        '''
        #start_time = time.time()  # å¼€å§‹è®¡æ—¶
        #problem_feat, mse_loss, _ = self.update_features(exer_id)
        #problem_feat = self.problem(exer_id)
        # æ‰¹é‡è·å–é—®é¢˜ç‰¹å¾
        
        exer_ids_list = exer_id.cpu().tolist() if isinstance(exer_id, torch.Tensor) else exer_id
        '''
        problem_feat = torch.stack([
            self.problem_features[pid] for pid in exer_ids_list
        ]).to(exer_id.device)
        print("problem_feat",problem_feat.shape)
        mse_loss = torch.tensor(0.0)
        if torch.cuda.current_device() == 0:
            check_tensor(problem_feat, "problem_feat")
        '''
        '''
        dim = 768
        max_len = max([self.problem_features[pid].shape[0] for pid in exer_ids_list])

        padded_feats = []
        for pid in exer_ids_list:
            feat = self.problem_features[pid]
            L = feat.shape[0]
            if L < max_len:
                # pad åˆ° max_len
                pad_len = max_len - L
                feat = F.pad(feat, (0, 0, 0, pad_len))  # åœ¨ç¬¬0ç»´åé¢ pad
            elif L > max_len:
                feat = feat[:max_len]  # æˆªæ–­
            padded_feats.append(feat)

        problem_feat = torch.stack(padded_feats).to(exer_id.device)  # [B, max_len, dim]
        print("problem_feat", problem_feat.shape)
        '''
        
        mse_loss = torch.tensor(0.0)
        #end_time = time.time()  # ç»“æŸè®¡æ—¶
        #batch_time = end_time - start_time
        #print(f"é—®é¢˜ç‰¹å¾æå– time: {batch_time:.4f} s")
        '''
        problem_feat_text = self.problem_text(exer_id)
        problem_feat_img = self.problem_img(exer_id)
        problem_feat = torch.cat([problem_feat_text, problem_feat_img], dim=1)  # [batch, 1024]
        '''
        '''
        # è®¡ç®—æ–‡æœ¬å’Œå›¾åƒç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦
        # problem_feat_text: [batch_size, 512]
        # problem_feat_img: [batch_size, 512]
        cosine_sim = F.cosine_similarity(problem_feat_text, problem_feat_img, dim=1)  # è¾“å‡º [batch_size]

        # æˆ‘ä»¬å¸Œæœ›ç›¸ä¼¼åº¦è¶Šå¤§è¶Šå¥½ï¼ˆæ¥è¿‘1ï¼‰ï¼Œæ‰€ä»¥æŸå¤± = 1 - å¹³å‡ç›¸ä¼¼åº¦
        mse_loss = 1 - cosine_sim.mean()
        '''
        
        '''
        # å‡è®¾ä¸€ä¸ªbatchä¸­æœ‰ problem_feat_text å’Œ problem_feat_img
        # 1. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        text_norm = F.normalize(problem_feat_text, dim=1)
        img_norm = F.normalize(problem_feat_img, dim=1)
        similarity_matrix = torch.mm(text_norm, img_norm.t()) # [batch_size, batch_size]

        # 2. ç›®æ ‡æ ‡ç­¾ï¼šå¯¹è§’çº¿ä¸Šçš„æ ·æœ¬æ˜¯æ­£æ ·æœ¬å¯¹
        labels = torch.arange(similarity_matrix.size(0)).to(similarity_matrix.device)
        temperature = 0.2
        # 3. è®¡ç®—å¯¹æ¯”æŸå¤±ï¼ˆå¯¹äºæ–‡æœ¬ç‰¹å¾æ¥è¯´ï¼Œå¯¹åº”çš„å›¾åƒç‰¹å¾æ˜¯å…¶æ­£æ ·æœ¬ï¼‰
        cont_loss_text = F.cross_entropy(similarity_matrix / temperature, labels)
        cont_loss_image = F.cross_entropy(similarity_matrix.t() / temperature, labels)
        mse_loss = (cont_loss_text + cont_loss_image) / 2
        '''
        
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(problem_feat, "problem_feat_from_update")
            self.print_memory("åœ¨æ›´æ–°ç‰¹å¾å")
        '''
        #difficulty_input = torch.cat([related_kn_feat, problem_feat], dim=1)
        #debug_print(difficulty_input, "difficulty_input_after_cat")
        

        #shared = self.shared_encoder(problem_feat)
        #debug_print(shared, "shared_after_encoder")

        #k_difficulty = self.diff_head(problem_feat)
        #k_difficulty = self.know_diff(problem_feat,kn_emb)
        k_difficulty =  torch.sigmoid(self.k_difficulty_NCDM(exer_id))
        
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(k_difficulty, "k_difficulty_after_head")
            self.print_memory("åœ¨éš¾åº¦ç‰¹å¾å")
        '''
        #e_discrimination = self.disc_head(problem_feat)
        #e_discrimination = self.know_disc(problem_feat,kn_emb)
        e_discrimination = torch.sigmoid(self.e_discrimination_NCDM(exer_id))*10
      
        '''
        problem_feat = F.layer_norm(problem_feat, problem_feat.shape[-1:])

        feat_transposed = problem_feat.transpose(1, 2)  # [batch_size, 768, 80]

        # åº”ç”¨é€‚é…å±‚ [batch_size, 768, 80] -> [batch_size, C, M]
        F_j = self.feature_adapter(feat_transposed)  # [batch_size, 512, 50]
        F_j = F.layer_norm(F_j, F_j.shape[-1:])

        if torch.cuda.current_device() == 0:
            check_tensor(feat_transposed, "feat_transposed")
            print(f"feat_transposed stats - min: {feat_transposed.min()}, max: {feat_transposed.max()}, has_nan: {torch.isnan(feat_transposed).any()}, has_inf: {torch.isinf(feat_transposed).any()}")

        if torch.cuda.current_device() == 0:
            check_tensor(F_j, "F_j")
            print(f"F_j stats - min: {F_j.min()}, max: {F_j.max()}, has_nan: {torch.isnan(F_j).any()}, has_inf: {torch.isinf(F_j).any()}")
            print(f"F_j shape: {F_j.shape}")
        
        
        
        #F_j = problem_feat
        
        # ========== 1. æ‰¹é‡å¤„ç†æ‰€æœ‰æ ·æœ¬ ==========
        batch_size = kn_emb.shape[0]
        total_knowledge = self.know_pro.weight.shape[0]

        # è·å–æ‰€æœ‰è€ƒå¯Ÿçš„çŸ¥è¯†ç‚¹
        batch_indices, knowledge_indices = torch.nonzero(kn_emb, as_tuple=True)

        if len(batch_indices) > 0:
            # åªé€‰å–è€ƒå¯Ÿçš„çŸ¥è¯†ç‚¹ç‰¹å¾
            selected_knowledge = self.know_pro.weight[knowledge_indices]  # [num_selected, 2048]
            
            # è®¡ç®—ä¸­é—´ç»“æœ
            intermediate = torch.matmul(selected_knowledge, self.W_p)  # [num_selected, 50]
            intermediate = F.layer_norm(intermediate, intermediate.shape[-1:])
            
            # ä¸ºæ¯ä¸ªé€‰ä¸­çš„çŸ¥è¯†ç‚¹è·å–å¯¹åº”çš„F_j
            selected_F_j = F_j[batch_indices]  # [num_selected, 512, 50]
            
            def compute_better_W_j(intermediate, selected_F_j):
                """è®ºæ–‡åŸå§‹æ–¹æ³• - ç›´æ¥çº¿æ€§å˜æ¢"""
                # åŸå§‹è®ºæ–‡æ–¹æ³•ï¼šç›´æ¥çŸ©é˜µä¹˜æ³•
                W_j_selected = torch.bmm(
                    selected_F_j,  # [num_selected, 512, 50]
                    intermediate.unsqueeze(-1)  # [num_selected, 50, 1]
                ).squeeze(-1)  # [num_selected, 512]
                
                print(f"åŸå§‹å…³ç³»çŸ©é˜µW_jèŒƒå›´: [{W_j_selected.min():.3f}, {W_j_selected.max():.3f}]")
                return W_j_selected

            # æ›¿æ¢åŸæ¥çš„è®¡ç®—
            W_j_selected = compute_better_W_j(intermediate, selected_F_j)*0.05

            # æ›´æ–°è¯Šæ–­å‡½æ•°
            def correct_diagnosis(W_j_selected):
                print("=== æ”¹è¿›çš„CMNCDå…³ç³»çŸ©é˜µè¯Šæ–­ ===")
                
                # 1. åŸºæœ¬ç»Ÿè®¡
                print(f"æ•°å€¼èŒƒå›´: [{W_j_selected.min():.3f}, {W_j_selected.max():.3f}]")
                print(f"å‡å€¼: {W_j_selected.mean():.3f}, æ ‡å‡†å·®: {W_j_selected.std():.3f}")
                
                # 2. æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒï¼ˆæ›´åˆç†çš„åˆ’åˆ†ï¼‰
                very_strong = (W_j_selected > 0.1).float().mean()    # æƒé‡>10%
                strong = ((W_j_selected > 0.05) & (W_j_selected <= 0.1)).float().mean()
                moderate = ((W_j_selected > 0.01) & (W_j_selected <= 0.05)).float().mean()
                weak = (W_j_selected <= 0.01).float().mean()         # æƒé‡<1%
                
                print("æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ:")
                print(f"  å¾ˆå¼º(>0.1): {very_strong:.3f}")
                print(f"  å¼º(0.05-0.1): {strong:.3f}") 
                print(f"  ä¸­ç­‰(0.01-0.05): {moderate:.3f}")
                print(f"  å¼±(<0.01): {weak:.3f}")
                
                # 3. æ£€æŸ¥ç¨€ç–æ€§
                sparsity = (W_j_selected < 0.001).float().mean()
                if sparsity > 0.9:
                    print(f"âš ï¸  è­¦å‘Š: æ³¨æ„åŠ›è¿‡äºç¨€ç– ({sparsity:.3f})")

            # ä½¿ç”¨æ”¹è¿›çš„è¯Šæ–­
            correct_diagnosis(W_j_selected)

            # ç»§ç»­ä½¿ç”¨æ³¨æ„åŠ›æƒé‡è®¡ç®—U_j_selected
            U_j_selected = torch.bmm(
                W_j_selected.unsqueeze(1),  # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡ [num_selected, 1, 512]
                selected_F_j                # [num_selected, 512, 50]
            ).squeeze(1)  # [num_selected, 50]

            U_j_selected = F.layer_norm(U_j_selected, U_j_selected.shape[-1:])
            

            
            # ========== æ–°å¢ï¼šéš¾åº¦é¢„æµ‹å¤´è¯¦ç»†æ£€æŸ¥ ==========
            print("=== éš¾åº¦é¢„æµ‹å¤´è¯¦ç»†æ£€æŸ¥ ===")
            
            #stu_b = stu_emb[batch_indices]
            #concat_feat = torch.cat([U_j_selected, stu_b], dim=-1)  # [B, D + num_kn]

            #print("stu_b = stu_emb[batch_indices]",stu_b.shape)
            print("U_j_selected",U_j_selected.shape)
            # æ£€æŸ¥çº¿æ€§å±‚è¾“å‡º
            linear_output = self.diff_head_k(U_j_selected)
            print(f"çº¿æ€§å±‚è¾“å‡º: min={linear_output.min():.4f}, max={linear_output.max():.4f}, mean={linear_output.mean():.4f}")
            
            # æ£€æŸ¥sigmoidåè¾“å‡º
            selected_difficulty = torch.sigmoid(linear_output)  # [num_selected, 1]
            #selected_difficulty = linear_output  # [num_selected, 1]
            print(f"Sigmoidå: min={selected_difficulty.min():.4f}, max={selected_difficulty.max():.4f}, mean={selected_difficulty.mean():.4f}")

            # ========== æ–°å¢ï¼šè‡ªç›‘ç£é‡æ„ ==========
            # é‡æ„æ¨¡æ€ç‰¹å¾
            reconstructed_flat = self.decoder(U_j_selected)  # [num_selected, 512*50]
            reconstructed_F_j = reconstructed_flat.view(selected_F_j.shape)  # [num_selected, 512, 50]

            # è®¡ç®—é‡æ„æŸå¤±ï¼ˆåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
            reconstruction_loss = F.mse_loss(reconstructed_F_j, selected_F_j)

            
            # åˆ†æ•£å›å®Œæ•´å‘é‡
            k_difficulty = torch.zeros(batch_size, total_knowledge, device=kn_emb.device)
            
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            selected_difficulty = selected_difficulty.to(k_difficulty.dtype)  # æ·»åŠ è¿™ä¸€è¡Œ
            
            k_difficulty[batch_indices, knowledge_indices] = selected_difficulty.squeeze(1)
            
            # æ£€æŸ¥ç‰¹å¾ä¼ é€’è¿‡ç¨‹ä¸­æ˜¯å¦ä¸¢å¤±äº†ä¿¡æ¯
            print("=== ç‰¹å¾ä¼ é€’æ£€æŸ¥ ===")
            print(f"problem_feat range: [{problem_feat.min():.3f}, {problem_feat.max():.3f}]")
            print(f"F_j range: [{F_j.min():.3f}, {F_j.max():.3f}]") 
            print(f"U_j_selected range: [{U_j_selected.min():.3f}, {U_j_selected.max():.3f}]")

            # æ¦‚å¿µç¼–ç å¯èƒ½æ²¡æœ‰å­¦åˆ°æœ‰æ„ä¹‰çš„è¡¨ç¤º
            selected_knowledge = self.know_pro.weight[knowledge_indices]
            print(f"æ¦‚å¿µç¼–ç èŒƒå›´: [{selected_knowledge.min():.3f}, {selected_knowledge.max():.3f}]")
            print(f"æ¦‚å¿µç¼–ç å‡å€¼: {selected_knowledge.mean():.3f}")
        '''
        '''
        # ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°txt
        if torch.cuda.current_device() == 0:
            # ä¿å­˜W_j_selected
            with open('W_j_selected.txt', 'w') as f:
                f.write(f"Shape: {W_j_selected.shape}\n")
                for i in range(W_j_selected.shape[0]):
                    for j in range(W_j_selected.shape[1]):
                        f.write(f"{W_j_selected[i,j].item():.6f} ")
                    f.write("\n")
            
            # ä¿å­˜intermediate
            with open('intermediate.txt', 'w') as f:
                f.write(f"Shape: {intermediate.shape}\n")
                for i in range(intermediate.shape[0]):
                    for j in range(intermediate.shape[1]):
                        f.write(f"{intermediate[i,j].item():.6f} ")
                    f.write("\n")
            
            # ä¿å­˜selected_F_jï¼ˆå‰3ä¸ªæ ·æœ¬ï¼Œé¿å…æ–‡ä»¶å¤ªå¤§ï¼‰
            with open('selected_F_j.txt', 'w') as f:
                f.write(f"Shape: {selected_F_j.shape}\n")
                for i in range(min(3, selected_F_j.shape[0])):
                    f.write(f"Sample {i}:\n")
                    for j in range(selected_F_j.shape[1]):
                        for k in range(selected_F_j.shape[2]):
                            f.write(f"{selected_F_j[i,j,k].item():.6f} ")
                        f.write("\n")
                    f.write("\n")

        

        # ä¿å­˜U_j_selected
        if torch.cuda.current_device() == 0:
            with open('U_j_selected.txt', 'w') as f:
                f.write(f"Shape: {U_j_selected.shape}\n")
                for i in range(U_j_selected.shape[0]):
                    for j in range(U_j_selected.shape[1]):
                        f.write(f"{U_j_selected[i,j].item():.6f} ")
                    f.write("\n")

        
        # ä¿å­˜linear_output
        if torch.cuda.current_device() == 0:
            with open('linear_output.txt', 'w') as f:
                f.write(f"Shape: {linear_output.shape}\n")
                for i in range(linear_output.shape[0]):
                    f.write(f"{linear_output[i,0].item():.6f}\n")

        

        # ä¿å­˜selected_difficulty
        if torch.cuda.current_device() == 0:
            with open('selected_difficulty.txt', 'w') as f:
                f.write(f"Shape: {selected_difficulty.shape}\n")
                for i in range(selected_difficulty.shape[0]):
                    f.write(f"{selected_difficulty[i,0].item():.6f}\n")

        print("æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°txtæ–‡ä»¶")
        
        if torch.cuda.current_device() == 0:
            check_tensor(k_difficulty, "k_difficulty_final")
            print(f"å¤„ç†å®Œæˆ - é€‰ä¸­çŸ¥è¯†ç‚¹æ•°é‡: {len(batch_indices)}")
        '''
        '''
        # ========== 1. çŸ¥è¯†ç‚¹ç‰¹å¾å‡†å¤‡ ==========
        knowledge_features = self.know_pro.weight  
        P_j = knowledge_features.unsqueeze(0) * kn_emb.unsqueeze(2) 
        if torch.cuda.current_device() == 0:
            check_tensor(P_j, "P_j")
      
        intermediate = torch.matmul(P_j, self.W_p)  # [batch_size, 329, 50]
        # æ·»åŠ å½’ä¸€åŒ–
        intermediate = F.layer_norm(intermediate, intermediate.shape[-1:])
        if torch.cuda.current_device() == 0:
            check_tensor(intermediate, "intermediate_after_matmul")
        # è®¡ç®—å…³ç³»çŸ©é˜µ: intermediate @ F_j^T
        W_j = torch.matmul(intermediate, F_j.transpose(1, 2))  # [batch_size, 329, 512]
       
        for i in range(3):  # æ£€æŸ¥å‰3ä¸ªæ ·æœ¬
            sample_W_j = W_j[i]  # [329, 512]
            
            # æ£€æŸ¥æ³¨æ„åŠ›æ˜¯å¦é›†ä¸­
            entropy = -torch.sum(sample_W_j * torch.log(sample_W_j + 1e-8), dim=-1)
            print(f"æ ·æœ¬{i} - æ³¨æ„åŠ›ç†µ: {entropy.mean():.3f} Â± {entropy.std():.3f}")
            
            # æ£€æŸ¥ç¨€ç–æ€§
            sparsity = (sample_W_j < 0.01).float().mean()
            print(f"æ ·æœ¬{i} - æ³¨æ„åŠ›ç¨€ç–æ€§: {sparsity:.3f}")

        if torch.cuda.current_device() == 0:
            check_tensor(W_j, "W_j")
       
        U_j = torch.matmul(W_j, F_j)  # [batch_size, 329, 50] â† æ³¨æ„è¿™é‡Œç»´åº¦å˜äº†ï¼
        # ç«‹å³æ·»åŠ æ•°å€¼ç¨³å®šå¤„ç†
        
        if torch.cuda.current_device() == 0:
            check_tensor(U_j, "U_j")

        U_j = F.layer_norm(U_j, U_j.shape[-1:])
        if torch.cuda.current_device() == 0:
            check_tensor(U_j, "U_jå½’ä¸€åŒ–å")
        
        # ========== 4. åªå¤„ç†å®é™…è€ƒå¯Ÿçš„çŸ¥è¯†ç‚¹ ==========
        batch_indices, knowledge_indices = torch.nonzero(kn_emb, as_tuple=True)

        # è·å–è€ƒå¯ŸçŸ¥è¯†ç‚¹çš„ç‰¹å®šç‰¹å¾
        selected_U_j = U_j[batch_indices, knowledge_indices]  # [num_selected, 50] â† ç»´åº¦å˜äº†ï¼
        if torch.cuda.current_device() == 0:
            check_tensor(selected_U_j, "selected_U_j")
            
            
        # ========== 5. éš¾åº¦é¢„æµ‹ ==========
        # åŸºäºçŸ¥è¯†ç‚¹ç‰¹å®šç‰¹å¾é¢„æµ‹éš¾åº¦
        selected_difficulty = torch.sigmoid(self.diff_head_k(selected_U_j))  # [num_selected, 1]
        if torch.cuda.current_device() == 0:
            check_tensor(selected_difficulty, "selected_difficulty_after_sigmoid")
        # åˆ†æ•£å›å®Œæ•´å‘é‡
        k_difficulty = torch.zeros_like(kn_emb, dtype=selected_difficulty.dtype)
        k_difficulty[batch_indices, knowledge_indices] = selected_difficulty.squeeze(1)
        if torch.cuda.current_device() == 0:
        '''
# å¦‚æœæ–¹å·®æŒç»­ä¸‹é™ï¼Œè¯´æ˜ä¿¡æ¯åœ¨ä¸æ–­ä¸¢å¤±
        # ========== 6. åŒºåˆ†åº¦é¢„æµ‹ ==========
        #selected_discrimination = torch.sigmoid(self.disc_head_k(selected_U_j)) * 10
        #e_discrimination = torch.zeros_like(kn_emb, dtype=selected_discrimination.dtype)
        #e_discrimination[batch_indices, knowledge_indices] = selected_discrimination.squeeze(1)     
        #if torch.cuda.current_device() == 0:
        #    check_tensor(e_discrimination, "e_discrimination_after_head")
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(e_discrimination, "e_discrimination_after_head")
            self.print_memory("åœ¨åŒºåˆ†åº¦ç‰¹å¾å")
        '''
        #K_delta = self.htspd(stu_emb, k_difficulty)
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(K_delta, "K_delta_after_htspd")
            self.print_memory("åœ¨HTSPDå")
        '''

        '''
        new_k_difficulty = k_difficulty - K_delta
        check_tensor(new_k_difficulty, "new_k_difficulty_after_sub")
        if torch.cuda.current_device() == 0:
            self.print_memory("åœ¨new_k_difficultyå")
        '''
        #new_e_discrimination = self.disc(e_discrimination, self.student_freq_tensor[stu_id])
        # åœ¨è®¡ç®— new_e_discrimination åæ·»åŠ é™åˆ¶
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(new_e_discrimination, "new_e_discrimination_after_disc")
            self.print_memory("åœ¨new_e_discriminationå")
        '''
        #k_difficulty = torch.sigmoid(k_difficulty+log_k_difficulty)
        #input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb
        input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb
        
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(input_x, "input_x_before_network")
            self.print_memory("åœ¨input_xå")
        '''
        #difficulty_gap = k_difficulty - stu_emb  # é¢˜å¯¹è¿™ä¸ªäººæ¥è¯´éš¾å¤šå°‘
        #bias = (self.beta * K_delta)  # beta æ˜¯ä¸€ä¸ªå¯è°ƒå‚æ•°
        #input_x = input_x + bias
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(input_x, "input_x_before_network")
            self.print_memory("åœ¨input_xå")
        '''
        '''
        input_x = new_e_discrimination * (stu_emb - new_k_difficulty) * kn_emb
        if torch.cuda.current_device() == 0:
            check_tensor(input_x, "input_x_before_network")
        '''
        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(input_x, "input_x_after_layer1")
            self.print_memory("åœ¨input_x_after_layer1å")
        '''
        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(input_x, "input_x_after_layer2")
            self.print_memory("åœ¨input_x_after_layer2å")
        '''
        output = self.prednet_full3(input_x)
        '''
        # åŒºåˆ†åº¦ * çŸ¥è¯†ç‚¹å‘é‡
        weighted_kn = e_discrimination * kn_emb    # [batch, dim]

        # å†…ç§¯ï¼šå­¦ç”Ÿèƒ½åŠ›å‘é‡ ä¸ (åŠ æƒçŸ¥è¯†ç‚¹)
        interaction = torch.sum(stu_emb * weighted_kn, dim=1, keepdim=True)  # [batch, 1]

        # logits = å†…ç§¯ - éš¾åº¦
        output = interaction - k_difficulty
        '''
        '''
        if torch.cuda.current_device() == 0:
            check_tensor(output, "final_output")
            self.print_memory("åœ¨final_outputå")
        '''    
        '''
        if self.sum % 600 == 0:
            # æ‰‹åŠ¨åˆ—å‡ºæ‰€æœ‰é¢˜ç›®å¯¹ï¼ˆ6ä¸ªé¢˜ç›®ä¸¤ä¸¤ç»„åˆï¼‰
            pairs = [
                (50, 131), (50, 30), 
                (131, 30), 
                (30, 318), 
                (318, 514), 
                (514, 408)
            ]

            file_path = "similarity_resultâ€”â€”1024.txt"
            exer_ids = exer_id.tolist() if isinstance(exer_id, torch.Tensor) else exer_id

            for id1, id2 in pairs:
                if id1 in exer_ids and id2 in exer_ids:
                    idx1 = exer_ids.index(id1)
                    idx2 = exer_ids.index(id2)

                    # æå–å››ç§ç‰¹å¾
                    prob1, prob2 = problem_feat[idx1], problem_feat[idx2]
                    #kn1, kn2 = related_kn_feat[idx1], related_kn_feat[idx2]
                    diff1, diff2 = k_difficulty[idx1], k_difficulty[idx2]
                    discr1, discr2 = e_discrimination[idx1], e_discrimination[idx2]

                    # è®¡ç®—ç›¸ä¼¼åº¦
                    prob_sim = F.cosine_similarity(prob1.unsqueeze(0), prob2.unsqueeze(0)).item()
                    #kn_sim = F.cosine_similarity(kn1.unsqueeze(0), kn2.unsqueeze(0)).item()
                    diff_sim = F.cosine_similarity(diff1.unsqueeze(0), diff2.unsqueeze(0)).item()
                    discr_sim = F.cosine_similarity(discr1.unsqueeze(0), discr2.unsqueeze(0)).item()

                    # å†™å…¥ç»“æœ
                    with open(file_path, 'a', encoding='utf-8') as f:
                        f.write(f"é¢˜ç›®å¯¹: ({id1}, {id2})\n")
                        f.write(f" é¢˜ç›®ç‰¹å¾ç›¸ä¼¼åº¦:        {prob_sim:.4f}\n")
                        #f.write(f" çŸ¥è¯†ç‚¹ç‰¹å¾ç›¸ä¼¼åº¦:      {kn_sim:.4f}\n")
                        f.write(f" éš¾åº¦ç‰¹å¾ç›¸ä¼¼åº¦:        {diff_sim:.4f}\n")
                        f.write(f" åŒºåˆ†åº¦ç‰¹å¾ç›¸ä¼¼åº¦:      {discr_sim:.4f}\n")
                        f.write("--------------------------------------------------\n")

                    print(f"[âˆš] å·²ä¿å­˜ ({id1}, {id2}) ç›¸ä¼¼åº¦åˆ° {file_path}")
        '''
        if not dist.is_initialized() or dist.get_rank() == 0:
            print("8888")
            if self.sum % 200 == 0:
                batch_size = stu_emb.shape[0]
                for i in range(batch_size):
                    if exer_id[i] in [446, 1117, 911, 885, 1522, 493]:
                        nonzero_idx = torch.nonzero(kn_emb[i], as_tuple=True)[0]

                        # å–å¯¹åº”çš„å€¼
                        stu_kn = stu_emb[i][nonzero_idx]
                        diff_kn = k_difficulty[i][nonzero_idx]

                        # è½¬æˆ list
                        stu_kn_list = stu_kn.detach().cpu().tolist()
                        diff_kn_list = diff_kn.detach().cpu().tolist()
                        nonzero_idx_list = nonzero_idx.detach().cpu().tolist()

                        # è¿½åŠ å†™å…¥ TXT
                        with open('output_z_text.txt', 'a') as f:
                            f.write(f"stu_id: {stu_id[i].item()}\n")
                            f.write(f"exer_id: {exer_id[i].item()}\n")
                            f.write(f"kn_index: {nonzero_idx_list}\n")
                            f.write(f"stu_kn: {stu_kn_list}\n")
                            f.write(f"diff_kn: {diff_kn_list}\n\n")  # åˆ†éš”ä¸åŒæ ·æœ¬

        if self.training:
            self.sum += 1
        #mse_loss = torch.mean((k_difficulty - k_difficulty.mean(dim=0)) ** 2)
        #mse_loss = reconstruction_loss
        return output,mse_loss   
    def print_memory(self,tag=""):
        allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        reserved = torch.cuda.memory_reserved() / 1024**2    # MB
        print(f"[{tag}] allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB")


        '''
        if d_only:
            with torch.no_grad():  # å†»ç»“ç”Ÿæˆå™¨å’Œä¸»ç½‘ç»œ
                related_kn_feat = torch.matmul(kn_emb, self.knowledge_feat)  # [batch_size, dim]

                stu_emb = torch.sigmoid(self.student_emb(stu_id))  # [batch_size, knowledge_n]
                

                stu_feat = self.stu_to_feat(stu_emb)  # [batch, 512]
                
                # æ­¥éª¤2: ä¸é¢˜ç›®ç‰¹å¾é€å…ƒç´ äº¤äº’
                weight = torch.sigmoid(
                    stu_feat * self.problem_feat[exer_id][:, :512]  # [batch, 512]
                ).unsqueeze(-1)  # [batch, 512, 1]

               
                # çŸ¥è¯†ç‚¹éš¾åº¦æƒé‡ï¼ˆåŒæ ·ä½¿ç”¨æ˜ å°„åçš„ stu_featï¼‰
                weight_know = torch.sigmoid(
                    stu_feat * related_kn_feat[:, :512]  # [batch, 512]
                ).unsqueeze(-1)  # [batch, 512, 1]

                weighted_feat = self.problem_feat[exer_id][:, :512] * weight.squeeze(-1)  # [batch, 512]
                

                # çŸ¥è¯†ç‚¹ç‰¹å¾åŠ æƒ
                weighted_kn_feat = related_kn_feat[:, :512] * weight_know.squeeze(-1)  # [batch, 512]
               

                # æ‹¼æ¥ç‰¹å¾ï¼ˆè¾“å‡ºå½¢çŠ¶ï¼š[batch, 1024]ï¼‰
                difficulty_input = torch.cat([weighted_feat, weighted_kn_feat], dim=1)  # [batch, 1024]
                discrimination_input = torch.cat([weighted_feat, weighted_kn_feat], dim=1)  # [batch, 1024]


            
                discrimination = self.e_discrimination(discrimination_input)
                
                k_difficulty = self.difficulty_net(difficulty_input)  # [batch, knowledge_n]
                delta = self.generator(stu_emb,difficulty_input)
                adjusted_difficulty = torch.clamp(k_difficulty + delta, min=0.0, max=1.0)

        else:
            related_kn_feat = torch.matmul(kn_emb, self.knowledge_feat)  # [batch_size, dim]

            stu_emb = torch.sigmoid(self.student_emb(stu_id))  # [batch_size, knowledge_n]
                

            stu_feat = self.stu_to_feat(stu_emb)  # [batch, 512]
                
                # æ­¥éª¤2: ä¸é¢˜ç›®ç‰¹å¾é€å…ƒç´ äº¤äº’
            weight = torch.sigmoid(
                    stu_feat * self.problem_feat[exer_id][:, :512]  # [batch, 512]
                ).unsqueeze(-1)  # [batch, 512, 1]

               
                # çŸ¥è¯†ç‚¹éš¾åº¦æƒé‡ï¼ˆåŒæ ·ä½¿ç”¨æ˜ å°„åçš„ stu_featï¼‰
            weight_know = torch.sigmoid(
                    stu_feat * related_kn_feat[:, :512]  # [batch, 512]
                ).unsqueeze(-1)  # [batch, 512, 1]

            weighted_feat = self.problem_feat[exer_id][:, :512] * weight.squeeze(-1)  # [batch, 512]
                

                # çŸ¥è¯†ç‚¹ç‰¹å¾åŠ æƒ
            weighted_kn_feat = related_kn_feat[:, :512] * weight_know.squeeze(-1)  # [batch, 512]
               

                # æ‹¼æ¥ç‰¹å¾ï¼ˆè¾“å‡ºå½¢çŠ¶ï¼š[batch, 1024]ï¼‰
            difficulty_input = torch.cat([weighted_feat, weighted_kn_feat], dim=1)  # [batch, 1024]
            discrimination_input = torch.cat([weighted_feat, weighted_kn_feat], dim=1)  # [batch, 1024]


            
            discrimination = self.e_discrimination(discrimination_input)
                
            k_difficulty = self.difficulty_net(difficulty_input)  # [batch, knowledge_n]
            delta = self.generator(stu_emb,difficulty_input)
            adjusted_difficulty = torch.clamp(k_difficulty + delta, min=0.0, max=1.0)


        # åŸæœ‰è¾“å…¥è®¡ç®—
        input_x = discrimination * (stu_emb - k_difficulty) * kn_emb
        
       
        # è¾“å…¥é¢„æµ‹ç½‘ç»œ
       
        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))
        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))
        
        # è¾“å‡ºé¢„æµ‹ç»“æœ
        output = self.prednet_full3(input_x)
       
        # å¯¹æŠ—æŸå¤±è®¡ç®—
        # å‡æ ·æœ¬ï¼šåŠ æ‰°åŠ¨ or åŸå§‹å€¼
        # å¯¹æŠ—ç›®æ ‡ï¼šè®© D åˆ¤åˆ«ä¸å‡ºæ¥ï¼ˆè®¤ä¸º G çš„æ˜¯çœŸï¼‰
        disc_input = torch.cat([stu_emb, adjusted_difficulty], dim=1)
        disc_score = self.discriminator(disc_input)
       
        if gcn_update:
            with open("debug_analysis.txt", "a") as f:
                for i in range(min(5, stu_emb.shape[0])):
                    correct = correct_id[i].item()
                    pred = output[i].item()

                    if correct == round(pred):
                        continue

                    ability = stu_emb[i].detach().cpu()                # [83]
                    base_difficulty = k_difficulty[i].detach().cpu()   # [83]
                    adj_difficulty = adjusted_difficulty[i].detach().cpu()  # [83]
                    kn_mask = kn_emb[i].detach().cpu()                 # [83]
                    disc = discrimination[i].detach().cpu()               # [83]

                    ab_kn = ability * kn_mask
                    base_diff_kn = base_difficulty * kn_mask
                    adj_diff_kn = adj_difficulty * kn_mask

                    gap = ability - base_difficulty
                    adj_gap = ability - adj_difficulty

                    disc_gap_kn = gap * disc * kn_mask
                    gap_kn = gap * kn_mask
                    adj_disc_gap_kn = adj_gap * disc * kn_mask
                    adj_gap_kn = adj_gap * kn_mask

                    non_zero_indices = kn_mask != 0  # [83]

                    f.write(f"âŒ Sample {i} (Prediction Error):\n")
                    f.write(f"  âœ… Correct Label: {correct}\n")
                    f.write(f"  ğŸ”® Predicted Output (Logit): {pred:.4f}\n")

                    f.write(f"  ğŸ’ª Ability Ã— kn_emb (non-zero positions):\n    {ab_kn[non_zero_indices].numpy()}\n")
                    f.write(f"  ğŸ§  Base Difficulty Ã— kn_emb (non-zero positions):\n    {base_diff_kn[non_zero_indices].numpy()}\n")
                    f.write(f"  ğŸ§¬ Adjusted Difficulty Ã— kn_emb (non-zero positions):\n    {adj_diff_kn[non_zero_indices].numpy()}\n")

                    f.write(f"  ğŸ§® (ability - base_difficulty) Ã— discrimination Ã— kn_emb (non-zero positions):\n    {disc_gap_kn[non_zero_indices].numpy()}\n")
                    f.write(f"  ğŸ§® (ability - base_difficulty) Ã— kn_emb (non-zero positions):\n    {gap_kn[non_zero_indices].numpy()}\n")

                    f.write(f"  ğŸ”§ (ability - adjusted_difficulty) Ã— discrimination Ã— kn_emb (non-zero positions):\n    {adj_disc_gap_kn[non_zero_indices].numpy()}\n")
                    f.write(f"  ğŸ”§ (ability - adjusted_difficulty) Ã— kn_emb (non-zero positions):\n    {adj_gap_kn[non_zero_indices].numpy()}\n")
                    f.write("-" * 60 + "\n")
        
        if(d_only):
            return   stu_emb.detach(),adjusted_difficulty.detach(),delta.detach()
        return output, adjusted_difficulty, disc_score

    
        # è·å–å­¦ç”ŸåµŒå…¥å‘é‡ï¼Œå¹¶é€šè¿‡ sigmoid æ¿€æ´»
        stu_emb = torch.sigmoid(self.student_emb(stu_id))
         # è·å–ç»ƒä¹ é¢˜çš„çŸ¥è¯†ç‚¹éš¾åº¦å‘é‡ï¼Œå¹¶é€šè¿‡ sigmoid æ¿€æ´»
        k_difficulty = torch.sigmoid(self.k_difficulty_NCDM(exer_id))
        # è·å–ç»ƒä¹ é¢˜çš„åŒºåˆ†åº¦ï¼Œå¹¶é€šè¿‡ sigmoid æ¿€æ´»åæ”¾å¤§ï¼ˆä¹˜ä»¥ 10ï¼‰
        e_discrimination = torch.sigmoid(self.e_discrimination_NCDM(exer_id)) * 10
        
        # è®¡ç®—é¢„æµ‹ç½‘ç»œçš„è¾“å…¥ï¼šçŸ¥è¯†ç‚¹å‘é‡ * (å­¦ç”Ÿå‘é‡ - çŸ¥è¯†ç‚¹éš¾åº¦å‘é‡) * åŒºåˆ†åº¦
        input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb
        
        # é€šè¿‡é¢„æµ‹ç½‘ç»œç¬¬ä¸€å±‚ï¼Œå¹¶åº”ç”¨ Dropout å’Œ sigmoid æ¿€æ´»
        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))
        # é€šè¿‡é¢„æµ‹ç½‘ç»œç¬¬äºŒå±‚ï¼Œå¹¶åº”ç”¨ Dropout å’Œ sigmoid æ¿€æ´»
        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))
        # é€šè¿‡é¢„æµ‹ç½‘ç»œè¾“å‡ºå±‚ï¼Œå¹¶åº”ç”¨ sigmoid æ¿€æ´»å¾—åˆ°æœ€ç»ˆè¾“å‡º
        output = torch.sigmoid(self.prednet_full3(input_x))


       
        if(d_only):
            return   stu_emb.detach(),k_difficulty.detach(),k_difficulty.detach()
        return output, k_difficulty.detach(),k_difficulty.detach()
        #return output
    '''
    



    def apply_clipper(self):
        """
        åº”ç”¨éè´Ÿæˆªæ–­ï¼ˆå°†ç½‘ç»œå‚æ•°é™åˆ¶ä¸ºéè´Ÿï¼‰ã€‚
        """
        clipper = NoneNegClipper()
        self.prednet_full1.apply(clipper)
        self.prednet_full2.apply(clipper)
        self.prednet_full3.apply(clipper)

    def get_knowledge_status(self, stu_id):
        """
        è·å–å­¦ç”Ÿçš„çŸ¥è¯†çŠ¶æ€ã€‚

        :param stu_id: LongTensor, å­¦ç”Ÿ ID çš„ç´¢å¼•
        :return: Tensor, å­¦ç”Ÿçš„çŸ¥è¯†çŠ¶æ€å‘é‡
        """
        stat_emb = torch.sigmoid(self.student_emb(stu_id))
        return stat_emb.data  # è¿”å›çŸ¥è¯†çŠ¶æ€

    def get_exer_params(self, exer_id):
        """
        è·å–ç»ƒä¹ é¢˜çš„å‚æ•°ï¼ˆçŸ¥è¯†ç‚¹éš¾åº¦å’ŒåŒºåˆ†åº¦ï¼‰ã€‚

        :param exer_id: LongTensor, ç»ƒä¹ é¢˜ ID çš„ç´¢å¼•
        :return: Tuple[Tensor, Tensor], åˆ†åˆ«ä¸ºçŸ¥è¯†ç‚¹éš¾åº¦å’ŒåŒºåˆ†åº¦
        """
        k_difficulty = torch.sigmoid(self.feature_to_difficulty(exer_id))
        e_discrimination = torch.sigmoid(self.feature_to_discrimination(exer_id)) * 10
        return k_difficulty.data, e_discrimination.data  # è¿”å›ç»ƒä¹ é¢˜å‚æ•°


class NoneNegClipper(object):
    """
    è‡ªå®šä¹‰çš„éè´Ÿæˆªæ–­å™¨ï¼Œç”¨äºç¡®ä¿æƒé‡å‚æ•°ä¸ºéè´Ÿå€¼ã€‚
    """
    def __init__(self):
        super(NoneNegClipper, self).__init__()

    def __call__(self, module):
        """
        é’ˆå¯¹æ¨¡å—çš„æƒé‡å‚æ•°ï¼Œåº”ç”¨éè´Ÿæˆªæ–­ã€‚

        :param module: nn.Module, éœ€è¦å¤„ç†çš„æ¨¡å—
        """
        if hasattr(module, 'weight'):  # æ£€æŸ¥æ¨¡å—æ˜¯å¦æœ‰ 'weight' å±æ€§
            w = module.weight.data
            # è®¡ç®—è´Ÿå€¼éƒ¨åˆ†ï¼ˆå¦‚æœå°äºé›¶åˆ™å–åï¼‰
            a = torch.relu(torch.neg(w))
            # è´Ÿå€¼éƒ¨åˆ†åŠ å›ï¼Œç¡®ä¿æƒé‡éè´Ÿ
            w.add_(a)