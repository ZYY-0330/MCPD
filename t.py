import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
# æ ‡å‡†åº“
import torch
import numpy as np
import random
import os

def set_seed(seed=42):
    # 1. åŸºæœ¬ Python éšæœºç§å­
    random.seed(seed)
    
    # 2. NumPy éšæœºç§å­
    np.random.seed(seed)
    
    # 3. PyTorch CPU éšæœºç§å­
    torch.manual_seed(seed)
    
    # 4. PyTorch GPU éšæœºç§å­ (é’ˆå¯¹å½“å‰æ˜¾å¡)
    torch.cuda.manual_seed(seed)
    
    # 5. PyTorch GPU éšæœºç§å­ (é’ˆå¯¹æ‰€æœ‰æ˜¾å¡ï¼Œé˜²æ­¢å¤šå¡è®­ç»ƒä¸ä¸€è‡´)
    torch.cuda.manual_seed_all(seed)
    
    # 6. ç¡®å®šæ€§ç®—æ³•é…ç½® (å…³é”®ï¼šè®© CuDNN çš„è¿ç®—ç»“æœä¹Ÿå›ºå®š)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # 7. è®¾ç½®ç¯å¢ƒå˜é‡ (é˜²æ­¢æŸäº›åº•å±‚åº“äº§ç”Ÿéšæœºæ€§)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"éšæœºç§å­å·²å›ºå®šä¸º: {seed}")

# åœ¨ä¸»æµç¨‹å¼€å§‹å‰è°ƒç”¨
set_seed(42)
import os
import time
from datetime import datetime
import itertools
import h5py
from torch.optim.lr_scheduler import CosineAnnealingLR
from NeuralNCDM import Net
import torch.multiprocessing as mp
from sklearn.metrics import f1_score

from torch.amp import autocast
# ç¬¬ä¸‰æ–¹åº“
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torch.cuda.amp import GradScaler
from torch import dist, optim, nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts
from torch.utils.tensorboard import SummaryWriter
from sklearn.manifold import TSNE
from itertools import chain
# æœ¬åœ°æ¨¡å—
from dataset import ProblemDataset,DistributedBalancedProblemBatchSampler # å…³é”®ä¿®æ”¹ç‚¹1ï¼šæ›¿æ¢æ•°æ®é›†ç±»
from BERT import MathBERTTextFeatureExtractor
from RestNet import FeatureExtractionModel
from configs.dataset_config import *
from fusion_model import HierarchicalFusionSystem
from torch.nn.modules.module import _addindent
from dataset import RelationBuilder, RecordDataset
from tqdm import tqdm
import h5py
import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
import logging
from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score
from torch.utils.data import DistributedSampler
import torch.nn.functional as F
from torch.utils.data import Subset
from UnifiedDataset import UnifiedDataset
import logging

import warnings
warnings.filterwarnings("ignore", message="adaptive_avg_pool2d_backward_cuda does not have a deterministic implementation")


torch.autograd.set_detect_anomaly(True)
logging.basicConfig(
    filename='train.log',  # æŒ‡å®šæ—¥å¿—æ–‡ä»¶
    level=logging.INFO,     # è®¾ç½®æ—¥å¿—çº§åˆ«
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
class EarlyStopper:
    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_metric = -np.inf

    def should_stop(self, current_metric):
        if current_metric > self.best_metric + self.min_delta:
            self.best_metric = current_metric
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False
class Trainer:
    def __init__(self, config, model, rank=0):
        self.config = config
        self.rank = rank
        self.model = model
        self._init_device()
        self._init_optimizer()
        self.scaler = torch.cuda.amp.GradScaler(init_scale=1024)  # æ··åˆç²¾åº¦æ¢¯åº¦ç¼©æ”¾
        self.best_metric = -float('inf')#è®°å½•æœ€é«˜åˆ†
        #self.loss_function = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_dir = os.path.join(OUTPUT_DIR, 'NCDM_logs', f'exp_{timestamp}')  
        self.writer = SummaryWriter(log_dir=log_dir)  
        self.step_sum = 0
    def _set_requires_grad(self, params, requires_grad):
        """æ‰¹é‡è®¾ç½®å‚æ•°æ¢¯åº¦çŠ¶æ€"""
        for p in params:
            p.requires_grad = requires_grad
    def _init_device(self):
        """è®¾å¤‡åˆå§‹åŒ–ï¼ˆæ”¯æŒå¤šGPUï¼‰"""
        if torch.cuda.is_available():
            self.device = torch.device(f'cuda:{self.rank}')
            self.model.to(self.device)
            

            if dist.is_initialized():
                self.model = DDP(
                    self.model,
                    device_ids=[self.rank],
                    find_unused_parameters=True,  # å¿…é¡»å¯ç”¨
                   
                    gradient_as_bucket_view=True  # æå‡æ•ˆç‡
                )
              
                               
        else:
            self.device = torch.device('cpu')
    def _init_optimizer(self):
        """
        [Phase 2 Final Strategy] åŒé‡åˆ†å±‚ç­–ç•¥ (å…¼å®¹ä¿å­˜ä»£ç ç‰ˆ)
        Group 0: Modal (LR=5e-5, WD=0.01)  -> å¯¹åº” TensorBoard LR_0
        Group 1: Base  (LR=1e-3, WD=1e-4)  -> å¯¹åº” TensorBoard LR_1
        """
        raw_model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # ==========================================
        # 1. å®šä¹‰è¶…å‚
        # ==========================================
        LR_BASE  = 1e-3      # åŸºç¡€å­¦ä¹ ç‡
        LR_MODAL = 1e-4    # æ¨¡æ€å­¦ä¹ ç‡ (5e-5)
        
        # ğŸŒŸ å·®å¼‚åŒ–æƒé‡è¡°å‡ (è¿™æ˜¯é‡ç‚¹!)
        WD_HIGH  = 1e-3         # å¼ºæ­£åˆ™ (ç»™æ¨¡æ€)
        WD_LOW   = 1e-3      # å¼±æ­£åˆ™ (ç»™åŸºç¡€)
        
        # ==========================================
        # 2. å®šä¹‰å‰ç¼€åˆ†ç»„
        # ==========================================
        MODAL_PREFIXES = (
            'model_feat',      # å›¾åƒ/æ–‡æœ¬èåˆå±‚
            'know_projector',  # çŸ¥è¯†ç‚¹æŠ•å½±
            'W_p',             # Attention å‚æ•°
            'gate',            # é—¨æ§å‚æ•°
            'diff_head',       # éš¾åº¦é¢„æµ‹å¤´
            'fusion'           # èåˆå±‚
        ) 
        
        modal_params = [] # å¯¹åº” Group 0
        base_params = []  # å¯¹åº” Group 1
        
        print(f"\nâš¡ [Optimizer] åˆå§‹åŒ– Phase 2 åŒé‡åˆ†å±‚æ¨¡å¼...")
        print(f"   >>> Group 0 (Modal): LR={LR_MODAL}, WD={WD_HIGH}")
        print(f"   >>> Group 1 (Base) : LR={LR_BASE},  WD={WD_LOW}")

        for name, param in raw_model.named_parameters():
            if not param.requires_grad:
                continue
            
            # 1. åˆ¤æ–­æ˜¯å¦ä¸è¡°å‡ (Bias/LayerNorm)
            no_decay_list = ['bias', 'LayerNorm.weight']
            if any(nd in name for nd in no_decay_list):
                real_wd = 0.0
            else:
                # 2. å¦‚æœä¸æ˜¯ Biasï¼Œåˆ™æ ¹æ®ç»„åˆ«å†³å®š WD
                if any(k in name for k in MODAL_PREFIXES):
                    real_wd = WD_HIGH  # æ¨¡æ€ç»„ç”¨ 0.01
                else:
                    real_wd = WD_LOW   # åŸºç¡€ç»„ç”¨ 0.0001

            # 3. åˆ†ç»„è£…å¡«
            if any(k in name for k in MODAL_PREFIXES):
                modal_params.append({
                    'params': param, 
                    'lr': LR_MODAL, 
                    'weight_decay': real_wd, # âœ… è¿™é‡Œä½¿ç”¨äº†å·®å¼‚åŒ– WD
                    'name': name,
                    'initial_lr': LR_MODAL
                })
            else:
                base_params.append({
                    'params': param, 
                    'lr': LR_BASE, 
                    'weight_decay': real_wd, # âœ… è¿™é‡Œä½¿ç”¨äº†å·®å¼‚åŒ– WD
                    'name': name,
                    'initial_lr': LR_BASE
                })

        # ==========================================
        # 3. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        # ==========================================
        # æ³¨æ„é¡ºåºï¼šmodal_params åœ¨å‰ (Group 0)ï¼Œbase_params åœ¨å (Group 1)
        # è¿™ä¸ä½ çš„ TensorBoard è®°å½•ä»£ç å®Œç¾å¯¹åº”
        self.optimizer = torch.optim.AdamW(
            modal_params + base_params,
            lr=LR_BASE, 
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-6
        )

        print(f"   >>> Group Modal Params: {len(modal_params)}")
        print(f"   >>> Group Base  Params: {len(base_params)}\n")
    '''
    def _init_optimizer(self):
        """
        åˆå§‹åŒ–åˆ†å±‚ä¼˜åŒ–å™¨ (Layer-wise LR)
        ç›®æ ‡: Attention è·‘æ…¢ (1e-4)ï¼ŒNCDM è·‘å¿« (1e-3)ã€‚
        """
        raw_model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # 1. å®šä¹‰å‚æ•°åˆ†ç»„å‰ç¼€
        FUSION_PREFIXES = ('model_feat','W_p', 'diff_head_k', 'know_pro',) 
        #'W_p', 'diff_head_k', 'know_pro'
        
        fusion_params = []   # ä½é€Ÿç»„ (1e-4)
        ncdm_params = []     # é«˜é€Ÿç»„ (1e-3)
        

        FUSION_TARGET_LR = self.config['learning_rate_1'] # 0.0001
        NCDM_TARGET_LR = HIGH_LR = 0.0007

        # 2. éå†å‚æ•°ï¼Œæ ¹æ®å‰ç¼€åˆ†é…é€Ÿåº¦
        for name, param in raw_model.named_parameters():
            if not param.requires_grad:
                continue
            
            # æƒé‡è¡°å‡åˆ†ç»„ (æ ‡å‡†æ“ä½œ)
            no_decay = ['bias', 'LayerNorm.weight']
            wd = self.config['weight_decay'] if not any(nd in name for nd in no_decay) else 0.0

            if name.startswith(FUSION_PREFIXES):
                # èåˆæ ¸å¿ƒï¼Œä½¿ç”¨ä½å­¦ä¹ ç‡ (0.0001)
                fusion_params.append({
                    'params': param, 
                    'lr': FUSION_TARGET_LR, 
                    'weight_decay': wd, 
                    'name': name,
                    'initial_lr': FUSION_TARGET_LR # å…³é”®ï¼šç”¨æ˜ç¡®çš„å˜é‡
                })
            else:
                # NCDM æ ¸å¿ƒ (Embeddings/Output)ï¼Œä½¿ç”¨é«˜å­¦ä¹ ç‡ (0.0007)
                ncdm_params.append({
                    'params': param, 
                    'lr': NCDM_TARGET_LR, # å…³é”®ï¼šä½¿ç”¨æ˜ç¡®çš„ NCDM ç›®æ ‡ LR å˜é‡
                    'weight_decay': wd, 
                    'name': name,
                    'initial_lr': NCDM_TARGET_LR # å…³é”®ï¼šç”¨æ˜ç¡®çš„å˜é‡
                })
        # 3. åˆå§‹åŒ–ä¼˜åŒ–å™¨ (å°†ä¸¤ä¸ªç»„éƒ½ä¼ å…¥)
        self.optimizer = torch.optim.AdamW(
            fusion_params + ncdm_params,
            #lr=self.config['learning_rate_1'], # è¿™é‡Œçš„å€¼ä¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬å¯¹æ¯ä¸ªç»„éƒ½è®¾ç½®äº† LR
            betas=(0.9, 0.999),
            eps=1e-8
        )
        if len(self.optimizer.param_groups) >= 2:
        # Group 0 (Fusion): ç¡®ä¿æ˜¯ 0.0001
            self.optimizer.param_groups[0]['lr'] = FUSION_TARGET_LR
            self.optimizer.param_groups[0]['initial_lr'] = FUSION_TARGET_LR # é‡æ–°èµ‹å€¼ initial_lr
            
            # Group 1 (NCDM): å¼ºåˆ¶è®¾ç½®ä¸º 0.0007
            self.optimizer.param_groups[1]['lr'] = NCDM_TARGET_LR 
            self.optimizer.param_groups[1]['initial_lr'] = NCDM_TARGET_LR # é‡æ–°èµ‹å€¼ initial_lr
            
        # 4. è°ƒåº¦å™¨ (Plateau)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.5,
            patience=5,
            verbose=True,
            min_lr=1e-6
        )

        self.freeze_model_feat = False
        final_fusion_lr = self.optimizer.param_groups[0]['initial_lr']
        final_ncdm_lr = self.optimizer.param_groups[1]['initial_lr']
        print(f"--- Optimizer Init Check ---")
        print(f"Config LR 1 (Fusion Target): {self.config.get('learning_rate_1')}")
        print(f"NCDM Target HIGH_LR: {HIGH_LR}")
        print(f"Group 0 (Fusion) Target: {final_fusion_lr}")
        print(f"Group 1 (NCDM) Target: {final_ncdm_lr}")
        print(f"--- Check End ---")

    '''
    '''
    def _init_optimizer(self):
        """ç»Ÿä¸€ä¼˜åŒ–å™¨é…ç½®ï¼Œä¸åˆ†æ¨¡å—"""

        raw_model = self.model.module if hasattr(self.model, 'module') else self.model

        # æ‰€æœ‰å‚æ•°éƒ½å‚ä¸ä¼˜åŒ–
        all_params = list(raw_model.named_parameters())

        # æƒé‡è¡°å‡é…ç½®ï¼ˆbiaså’ŒLayerNormæƒé‡å•ç‹¬å¤„ç†ï¼‰
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            # ç¬¬ä¸€ç»„ï¼šæ™®é€šçš„æƒé‡ (Weight)ï¼Œéœ€è¦æƒé‡è¡°å‡ (é˜²æ­¢è¿‡æ‹Ÿåˆ)
            {
                'params': [p for n, p in all_params if not any(nd in n for nd in no_decay)],
                'weight_decay': self.config['weight_decay']  # é€šå¸¸æ˜¯0.01 æˆ– 0.05
            },
            # ç¬¬äºŒç»„ï¼šåç½®é¡¹ (Bias) å’Œ LayerNormï¼Œä¸éœ€è¦æƒé‡è¡°å‡ (è¿™æ˜¯ä¸šç•Œæ ‡å‡†åšæ³•)
            {
                'params': [p for n, p in all_params if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]

        # åˆå§‹åŒ–AdamWä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.config['learning_rate_1'],
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.5,
            patience=5,    # å¿è€ 5 ä¸ª Epoch ä¸æ¶¨å†é™
            verbose=True,
            min_lr=1e-6    # æœ€å°ä¸ä½äºè¿™ä¸ªæ•°
        )
        

        self.freeze_model_feat = False

    def _init_optimizer_without_model_feat(self):
        """å†»ç»“ model_featï¼Œä»…ä¼˜åŒ–å…¶ä»–æ¨¡å—"""
        raw_model = self.model.module if hasattr(self.model, 'module') else self.model

       
        no_decay = ['bias', 'LayerNorm.weight']
        
        # åªä¿ç•™ä¸å±äº model_feat çš„å‚æ•°ï¼ˆå·²ç»å†»ç»“äº†ï¼‰
        # åŠ ä¸Š module.
        all_params = [
            (n, p) for n, p in raw_model.named_parameters()
            if not n.startswith('module.model_feat')
        ]


        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in all_params if not any(nd in n for nd in no_decay)],
                'weight_decay': self.config['weight_decay']
            },
            {
                'params': [p for n, p in all_params if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]

        self.optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.config['learning_rate_2'],
            betas=(0.9, 0.999),
            eps=1e-8
        )

        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',    # æˆ‘ä»¬å¸Œæœ› AUC è¶Šé«˜è¶Šå¥½
            factor=0.5,    # æ¶¨ä¸åŠ¨äº†å°± å­¦ä¹ ç‡ * 0.5
            patience=5,    # å¿è€ 5 ä¸ª Epoch ä¸æ¶¨å†é™
            verbose=True,
            min_lr=1e-6    # æœ€å°ä¸ä½äºè¿™ä¸ªæ•°
        )

    '''

    def print_memory(self,tag=""):
        allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        reserved = torch.cuda.memory_reserved() / 1024**2    # MB
        print(f"[{tag}] allocated: {allocated:.2f} MB, reserved: {reserved:.2f} MB")
    def count_active_trainable_params(self,model, active_prefix='model_feat'):
        """
        è®¡ç®—å¹¶åˆ†å‰²åœ¨å½“å‰ Batch ä¸­è·å¾—äº†æ¢¯åº¦çš„å¯è®­ç»ƒå‚æ•°é‡ã€‚
        :param model: DDP æˆ–åŸå§‹æ¨¡å‹å®ä¾‹ã€‚
        :param active_prefix: ä½ çš„ Fusion æ¨¡å—å‰ç¼€ (ä¾‹å¦‚ 'model_feat')ã€‚
        """
        if not dist.is_initialized() or dist.get_rank() == 0:
            raw_model = model.module if hasattr(model, 'module') else model
            total_active_params = 0
            active_fusion_sum = 0
            active_ncdm_sum = 0
            
            NCDM_PREFIXES = ('student_emb', 'k_difficulty_NCDM', 'e_discrimination_NCDM', 'output_layer')
            
            for name, param in raw_model.named_parameters():
                # 1. å¿…é¡»æ˜¯å¯è®­ç»ƒçš„å‚æ•° AND 2. å¿…é¡»æœ‰æ¢¯åº¦ (å³è¢« forward/backward æµç¨‹ç”¨åˆ°)
                if param.requires_grad and param.grad is not None:
                    param_count = param.numel()
                    total_active_params += param_count
                    
                    # åŒºåˆ†æ˜¯ NCDM åŸºç¡€å±‚è¿˜æ˜¯ Fusion å±‚
                    if name.startswith(NCDM_PREFIXES):
                        active_ncdm_sum += param_count
                    elif name.startswith(active_prefix):
                        active_fusion_sum += param_count

            # æ‰“å°ç»“æœ
            print("\n" + "="*80)
            print("ğŸ’¡ æ´»è·ƒå‚æ•°é‡åˆ†å‰²æŠ¥å‘Š (å®é™…å‚ä¸æœ¬ Batch è®­ç»ƒçš„å‚æ•°)")
            print("="*80)
            print(f"æ€»å¯è®­ç»ƒå‚æ•° (Total Trainable): {sum(p.numel() for p in raw_model.parameters() if p.requires_grad):,}")
            print("-" * 80)
            print(f"1. è·å¾—æ¢¯åº¦çš„æ€»å‚æ•° (Active): {total_active_params:,}")
            print(f"2. NCDM åŸºç¡€å‚æ•° (Active): {active_ncdm_sum:,}")
            print(f"3. Fusion/Attention ç³»ç»Ÿ (Active): {active_fusion_sum:,}")
            print("-" * 80)
            print(f"   => æ´»è·ƒçš„ Fusion å‚æ•°å æ€»æ´»è·ƒå‚æ•°çš„æ¯”ä¾‹: {active_fusion_sum / total_active_params * 100:.2f}%")
            print("="*80)
    def _apply_phase_strategy(self, epoch):
        """
        [æˆ˜ç•¥æ ¸å¿ƒ] å±€éƒ¨å¾®è°ƒç­–ç•¥ (Partial Fine-tuning)
        
        ç­–ç•¥é€»è¾‘ï¼š
        1. å…¨å±€åŸåˆ™ï¼šä¸ºäº†çœæ˜¾å­˜ï¼ŒBERT/ResNet çš„åº•å±‚ (Bottom Layers) æ°¸è¿œé”æ­»ã€‚
        2. å±€éƒ¨æ”¾å¼€ï¼šBERT Layer 11 (é¡¶å±‚) å’Œ ResNet Layer 4 (é¡¶å±‚) å…è®¸è®­ç»ƒã€‚
        3. Phase 1 (Epoch 0): é”æ­» NCDMï¼Œå¼ºè¿«æ¨¡æ€é¡¶å±‚å’ŒæŠ•å½±å±‚å­¦ä¹ ã€‚
        4. Phase 2 (Epoch 1+): å…¨å‘˜è§£å†» (é™¤äº†åº•å±‚éª¨å¹²)ã€‚
        """
        # å®šä¹‰é˜¶æ®µé˜ˆå€¼ (åªè·‘ 1 ä¸ª epoch çƒ­èº«è¶³å¤Ÿäº†)
        PHASE_1_EPOCHS = 2
        
        # 1. ç™½åå•ï¼šå±äº Fusion ä½“ç³»çš„ç»„ä»¶
        FUSION_KEYWORDS = [
            'model_feat', 'diff_head', 'W_p', 'know_pro', 
            'output_layer', 'img_proj', 'text_proj', 'gate_weight','snr_diff_head',
        ]
        
        # 2. æ·±å±‚å†»ç»“é»‘åå• (Deep Freeze List)
        # è¿™é‡Œçš„å±‚æ°¸è¿œä¸è®­ç»ƒï¼Œç”¨æ¥çœæ˜¾å­˜ + ä¿æŒåŸºç¡€ç‰¹å¾
        DEEP_FREEZE_KEYWORDS = [
            # ResNet åº•å±‚ (é” 1, 2, 3 å±‚; æ”¾ layer4)
            'img_feature.backbone.conv1',
            'img_feature.backbone.bn1',
            'img_feature.backbone.layer1',
            'img_feature.backbone.layer2',
            'img_feature.backbone.layer3', 
            'img_feature.backbone.layer4', 
            # BERT åº•å±‚ (é” 0-10 å±‚; æ”¾ layer.11)
            'text_feature.bert_model.embeddings',
            'text_feature.bert_model.encoder.layer.0.',
            'text_feature.bert_model.encoder.layer.1.',
            'text_feature.bert_model.encoder.layer.2.',
            'text_feature.bert_model.encoder.layer.3.',
            'text_feature.bert_model.encoder.layer.4.',
            'text_feature.bert_model.encoder.layer.5.',
            'text_feature.bert_model.encoder.layer.6.',
            'text_feature.bert_model.encoder.layer.7.',
            'text_feature.bert_model.encoder.layer.8.',
            'text_feature.bert_model.encoder.layer.9.',
            'text_feature.bert_model.encoder.layer.10.', 
            'text_feature.bert_model.encoder.layer.11.', 
        ]

        raw_model = self.model.module if hasattr(self.model, 'module') else self.model
        
        counts = {"frozen": 0, "training": 0}

        # --- æ‰§è¡Œç­–ç•¥ ---
        for name, param in raw_model.named_parameters():
            # é»˜è®¤ï¼šæ ¹æ® Phase å†³å®šæ˜¯å¦å¼€å¯
            should_train = True
            
            # è§„åˆ™ A: Phase 1 åªè®­æ¨¡æ€
            if epoch < PHASE_1_EPOCHS:
                # å¦‚æœä¸æ˜¯ Fusion ç»„ä»¶ï¼Œå°±é”æ­» (é” NCDM)
                if not any(k in name for k in FUSION_KEYWORDS):
                    should_train = False
            
            # è§„åˆ™ B: æ— è®ºä½•æ—¶ï¼Œåº•å±‚éª¨å¹²æ°¸è¿œé”æ­» (ä¸€ç¥¨å¦å†³)
            if any(k in name for k in DEEP_FREEZE_KEYWORDS):
                should_train = False
            
            # åº”ç”¨è®¾ç½®
            param.requires_grad = should_train
            
            if should_train: counts["training"] += param.numel()
            else: counts["frozen"] += param.numel()

        # --- ä»…ä¸»è¿›ç¨‹æ‰“å°çŠ¶æ€ ---
        if self.rank == 0:
            phase_name = "Phase 1: Modality Awakening (é” NCDM)" if epoch < PHASE_1_EPOCHS else "Phase 2: Joint Optimization (å…¨å¼€)"
            print(f"\n{'='*60}")
            print(f"ğŸš€ [Epoch {epoch}] å±€éƒ¨å¾®è°ƒç­–ç•¥æ‰§è¡Œ: {phase_name}")
            print(f"   >>> è®­ç»ƒå‚æ•°é‡: {counts['training']:,} | å†»ç»“å‚æ•°é‡: {counts['frozen']:,}")
            print(f"{'='*60}")
            
            # ğŸ” æŠ½æŸ¥å…³é”®å±‚çŠ¶æ€
            print("ğŸ” å…³é”®å±‚æŠ½æŸ¥:")
            check_points = [
                ('BERT Bottom', 'text_feature.bert_model.encoder.layer.0.'), # åº”é”
                ('BERT Top',    'text_feature.bert_model.encoder.layer.11.'),# Phase 1 åº”è®­
                ('ResNet Bott', 'img_feature.backbone.layer1'),              # åº”é”
                ('ResNet Top',  'img_feature.backbone.layer4'),              # Phase 1 åº”è®­
                ('Diff Head',   'diff_head'),                                # Phase 1 åº”è®­
                ('Student Emb', 'student_emb')                               # Phase 1 é”, Phase 2 è®­
            ]
            for tag, key in check_points:
                found = False
                for name, param in raw_model.named_parameters():
                    if key in name:
                        status = "âœ… è®­ç»ƒ" if param.requires_grad else "ğŸ”’ å†»ç»“"
                        print(f"   - {tag:<12}: {status} ({name[:25]}...)")
                        found = True
                        break
                if not found: print(f"   - {tag:<12}: âš ï¸ æœªæ‰¾åˆ°")
            print(f"{'='*60}\n")
    def train_epoch(self, train_loader, epoch):
        """å•epochè®­ç»ƒï¼ˆæ”¯æŒæ··åˆç²¾åº¦+åˆ†å¸ƒå¼ï¼‰"""
        self.model.train()
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        gcn_update = False

        # æ–°å¢ï¼šç”¨äºè®¡ç®— RMSE å’Œ AUC
        all_targets = []
        all_preds = []
        all_probs = []

        # è¿›åº¦æ¡é…ç½®ï¼ˆä»…ä¸»è¿›ç¨‹æ˜¾ç¤ºï¼‰
        progress_bar = tqdm(train_loader, 
                          desc=f"Epoch {epoch+1} [Rank {self.rank}]",
                          disable=not (self.rank == 0))
        

        # å®šä¹‰ TensorBoard æ—¥å¿—å­˜å‚¨ç›®å½•
        
      


        # ğŸš¨ Warmup è¶…å‚æ•°è®¾ç½® (è§£å†³ 1e-4 å¯åŠ¨å¤±è´¥çš„é—®é¢˜)
        WARMUP_STEPS = 50  # å‡è®¾å‰ 500 ä¸ª Batch è¿›è¡Œçƒ­èº«
        INITIAL_LR_FACTOR = 1e-3 # ä» 1e-3 * base_lr å¼€å§‹
        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=True):
            for batch_idx, batch in enumerate(progress_bar):
            
                
                # --- å…¼å®¹åˆ—è¡¨çš„æ•°æ®æ¬è¿ ---
                device = f'cuda:{self.rank}'
                new_batch = {}
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor):
                        # å¦‚æœæ˜¯ Tensorï¼Œç›´æ¥æ¬è¿
                        new_batch[k] = v.to(device)
                    elif isinstance(v, list):
                        # âœ… å¦‚æœæ˜¯åˆ—è¡¨ (æ¯”å¦‚ img_raw)ï¼Œéå†åˆ—è¡¨é‡Œçš„æ¯ä¸ª Tensor æ¬è¿
                        new_batch[k] = [x.to(device) for x in v if isinstance(x, torch.Tensor)]
                    else:
                        # å…¶ä»–ç±»å‹ (å¦‚å­—ç¬¦ä¸²) ä¿æŒåŸæ ·
                        new_batch[k] = v
                batch = new_batch
                validate_device_consistency(batch, self.model)
                
                global_step = batch_idx + epoch * len(train_loader)
                
                if global_step < WARMUP_STEPS:
                    # çº¿æ€§çˆ¬å‡å› å­ï¼šä» 0 (æˆ–æå°å€¼) çˆ¬å‡åˆ° 1.0
                    climbing_factor = (global_step + 1) / WARMUP_STEPS
                    
                    for i, param_group in enumerate(self.optimizer.param_groups):
                        # ğŸš¨ ä¿®æ­£åçš„ä»£ç ï¼šä½¿ç”¨ä¿å­˜çš„ 'initial_lr' ä½œä¸ºåŸºå‡†
                        base_lr = param_group['initial_lr'] 
                        
                        # è®¡ç®—å½“å‰æ­¥çš„ LR: ä» base_lr * 1e-3 çˆ¬å‡åˆ° base_lr
                        start_lr = base_lr * INITIAL_LR_FACTOR
                        
                        # çº¿æ€§çˆ¬å‡å› å­
                        climbing_factor = (global_step + 1) / WARMUP_STEPS
                        
                        # æ›´æ–° param_group['lr']
                        param_group['lr'] = start_lr + (base_lr - start_lr) * climbing_factor

                # ç„¶ååœ¨æ‚¨çš„è®­ç»ƒæ­¥éª¤ä»£ç ä¸­ï¼Œç”¨ try-except åŒ…è£¹æ•´ä¸ªæ­¥éª¤ï¼š
                try:
                
                    self.optimizer.zero_grad()
                    # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
                    with autocast(device_type='cuda',dtype=torch.float16):
                        # --- [è®¡æ—¶] Forward å¼€å§‹ ---
                        t_fwd_start = time.time()
                        #self.print_memory("åœ¨forwardå‰")
                        # å‰å‘ä¼ æ’­ï¼Œè·å–æ¨¡å‹è¾“å‡º
                        output_1, pred_id, pred_img, alpha = self.model.forward(
                            batch
                        ) 
                    
                        targets = batch['corrects'].squeeze().float()
                        
                        # ç¡®ä¿è¾“å‡ºç»´åº¦å¯¹é½ [Batch_Size]
                        output_fused = output_1.squeeze()
                        #output_id    = pred_id.squeeze()
                        #output_img   = pred_img.squeeze()

                        # 3. è®¡ç®—ä¸‰ä¸ªç‹¬ç«‹çš„ BCE Loss
                        # (A) ä¸» Loss: èåˆåçš„ç»“æœ (åŸæœ¬çš„ main_loss)
                        loss_main = F.binary_cross_entropy_with_logits(output_fused, targets)

                        # (B) ID è¾…åŠ© Loss: å¼ºè¿« ID åˆ†æ”¯ä¿æŒ 0.78 çš„æ°´å‡†
                        #loss_id   = F.binary_cross_entropy_with_logits(output_id, targets)

                        # (C) æ¨¡æ€ è¾…åŠ© Loss: å¼ºè¿«å›¾åƒåˆ†æ”¯å¿…é¡»è‡ªå·±å­¦ä¼šé¢„æµ‹ï¼(è¿™æ˜¯é‡ç‚¹)
                        #loss_img  = F.binary_cross_entropy_with_logits(output_img, targets)

                        # 4. åŠ æƒæ±‚å’Œ (æ ¸å¿ƒä¿®æ”¹)
                        # å»ºè®®æƒé‡: 
                        # - main: 1.0 (ä¸»ä»»åŠ¡)
                        # - id:   0.5 (IDå­¦å¾—å¿«ï¼Œç»™å°ç‚¹æƒé‡å³å¯)
                        # - img:  1.0 (å›¾åƒå­¦å¾—æ…¢ï¼Œç»™å¤§æƒé‡é€¼å®ƒå­¦ï¼Œæ›¿ä»£ä½ åŸæ¥çš„ mse_loss)
                        loss = loss_main + 0.1*alpha
                        
                        # --- æ­£ç¡®è®¡ç®—æ¦‚ç‡ ---
                        probs = torch.sigmoid(output_fused)  # å½¢çŠ¶ [batch_size]
                        preds = (probs >= 0.5).long()  # äºŒå€¼åŒ–é¢„æµ‹

                        #probs = output_1.squeeze(1)
                        #preds = (probs >= 0.5).long()  # shape [512]

                
                    # åå‘ä¼ æ’­
                    self.scaler.scale(loss).backward()
                

                    #if batch_idx == 0 and epoch == 0: # åªåœ¨ç¬¬ä¸€è½®çš„ç¬¬ä¸€ä¸ª Batch æ£€æŸ¥
                        # è°ƒç”¨ä¸Šé¢çš„å‡½æ•°
                    #    self.count_active_trainable_params(self.model)
                    self.scaler.unscale_(self.optimizer)

                    if self.step_sum % 1000 == 0:
                        # ç¡®ä¿ä¼ å…¥çš„æ˜¯ self.model
                        self.comprehensive_gradient_analysis(self.model, self.scaler)
                    
                    self.step_sum += 1


                    original_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    # 4. [è®°å½•] è®°å½•å„é¡¹æŒ‡æ ‡ (åªåœ¨ä¸»è¿›ç¨‹)
                    if self.rank == 0:
                        # è®¡ç®—å…¨å±€æ­¥æ•°
                        global_step = batch_idx + epoch * len(progress_bar)
                        
                        # è®°å½• Loss
                        #self.writer.add_scalar('Batch/Total_Loss', loss.item(), global_step)
                        #self.writer.add_scalar('Batch/Main_Loss', loss_main.item(), global_step)
                        #self.writer.add_scalar('Batch/loss_id', loss_id.item(), global_step)
                        #self.writer.add_scalar('Batch/loss_img', loss_img.item(), global_step)
                        #self.writer.add_scalar('Batch/alpha', alpha.item(), global_step)
                       
                        
                        # è®°å½•å­¦ä¹ ç‡
                        # 1. è·å–å„ç»„å½“å‰ LR
                        # ----------------------------------------------------
                        # ç»„ 0: æ¨¡æ€èåˆç»„ (Modal/Fusion)
                        lr_modal = self.optimizer.param_groups[0]['lr'] if len(self.optimizer.param_groups) > 0 else 0.0

                        # ç»„ 1: NCDM åŸºç¡€ç»„ (Base/ID)
                        lr_base = self.optimizer.param_groups[1]['lr'] if len(self.optimizer.param_groups) > 1 else 0.0

                        # ----------------------------------------------------
                        # 2. è®°å½•åˆ° TensorBoard
                        # ----------------------------------------------------

                        # ç»„ 0: æ¨¡æ€èåˆç»„ (Modal/Fusion)
                        self.writer.add_scalar('Batch/LR_0_Modal_Fusion', lr_modal, global_step)

                        # ç»„ 1: NCDM åŸºç¡€ç»„ (Base/ID)
                        self.writer.add_scalar('Batch/LR_1_Base_NCDM', lr_base, global_step)

                        # è®°å½•æ¢¯åº¦èŒƒæ•° (ç›´æ¥ç”¨ clip è¿”å›çš„åŸå§‹å€¼ï¼Œæ—¢å‡†ç¡®åˆçœäº†è®¡ç®—èµ„æº)
                        self.writer.add_scalar('Batch/Gradient_Norm', original_norm.item(), global_step)

                    self.scaler.step(self.optimizer)
                    with torch.no_grad():
                        for name, param in self.model.named_parameters():
                            if "prednet_full" in name and "weight" in name:
                                # å¼ºåˆ¶å°†æ‰€æœ‰è´Ÿæƒé‡ç›´æ¥å½’é›¶ï¼Œå½¢æˆâ€œç¡¬å¢™â€
                                param.clamp_(min=0.0)
                                #param.copy_(param.abs())

                    self.scaler.update()

                
                except Exception as e:
                    print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
                    import traceback
                    print("è¯¦ç»†å †æ ˆè·Ÿè¸ª:")
                    print(traceback.format_exc())
                    
                    # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
                    print("\n=== è°ƒè¯•ä¿¡æ¯ ===")
                    print(f"Epoch: {epoch}, Batch index: {batch_idx}")
                    
                    # æ£€æŸ¥è¾“å…¥æ•°æ®
                    print("\nè¾“å…¥æ•°æ®ç»Ÿè®¡:")
                    for key, value in batch.items():
                        if torch.is_tensor(value):
                            print(f"{key}: shape={value.shape}, min={value.min().item()}, max={value.max().item()}, "
                                f"has_nan={torch.isnan(value).any().item()}, has_inf={torch.isinf(value).any().item()}")
                    
                    # æ£€æŸ¥æ¨¡å‹è¾“å‡º
                    if 'output_1' in locals():
                        print(f"\noutput_1: shape={output_1.shape}, min={output_1.min().item()}, max={output_1.max().item()}, "
                            f"has_nan={torch.isnan(output_1).any().item()}, has_inf={torch.isinf(output_1).any().item()}")
                    
                    if 'output' in locals():
                        print(f"output: shape={output.shape}, min={output.min().item()}, max={output.max().item()}, "
                            f"has_nan={torch.isnan(output).any().item()}, has_inf={torch.isinf(output).any().item()}")
                    
                    # ä¿®å¤ mse_loss çš„æ£€æŸ¥
                    if 'mse_loss' in locals() and mse_loss is not None:
                        if torch.is_tensor(mse_loss):
                            print(f"mse_loss: value={mse_loss.item()}, has_nan={torch.isnan(mse_loss).any().item()}, "
                                f"has_inf={torch.isinf(mse_loss).any().item()}")
                        else:
                            print(f"mse_loss: value={mse_loss} (not a tensor)")

                    # ä¿®å¤ total_loss çš„æ£€æŸ¥
                    if 'total_loss' in locals():
                        if torch.is_tensor(total_loss):
                            print(f"total_loss: value={total_loss.item()}, has_nan={torch.isnan(total_loss).any().item()}, "
                                f"has_inf={torch.isinf(total_loss).any().item()}")
                        else:
                            print(f"total_loss: value={total_loss} (not a tensor)")
                    
                    if 'main_loss' in locals():
                        print(f"main_loss: value={main_loss.item()}, has_nan={torch.isnan(main_loss).any().item()}, "
                            f"has_inf={torch.isinf(main_loss).any().item()}")

                    
                    # æ£€æŸ¥æ¨¡å‹å‚æ•°
                    print("\næ¨¡å‹å‚æ•°æ¢¯åº¦ç»Ÿè®¡:")
                    for name, param in self.model.named_parameters():
                        if param.grad is not None:
                            grad_norm = param.grad.norm().item()
                            grad_has_nan = torch.isnan(param.grad).any().item()
                            grad_has_inf = torch.isinf(param.grad).any().item()
                            print(f"{name}: grad_norm={grad_norm}, has_nan={grad_has_nan}, has_inf={grad_has_inf}")
                    
                    # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥åœæ­¢è®­ç»ƒ
                    raise


                
            
                # --- ç´¯ç§¯æŒ‡æ ‡ ---
                batch_loss = loss.item()
                batch_acc = (preds == targets.long()).float().mean().item()
                batch_rmse = np.sqrt(mean_squared_error(targets.cpu().numpy(), probs.detach().cpu().numpy()))
                batch_auc = roc_auc_score(targets.cpu().numpy(), probs.detach().cpu().numpy())

                total_loss += batch_loss * targets.size(0)
                total_samples += targets.size(0)
                correct_predictions += (preds == targets.long()).sum().item()
                all_targets.extend(targets.long().cpu().numpy().flatten())
                all_probs.extend(probs.detach().cpu().numpy().flatten())
                #self.print_memory("åœ¨è®¡ç®—all_targetså’Œall_probså")
                # --- æ›´æ–°è¿›åº¦æ¡ ---
            
                if self.rank == 0:
                    progress_bar.set_postfix({
                        'loss': f"{batch_loss:.6f}",
                        'acc': f"{batch_acc:.6f}",
                        'RMSE': f"{batch_rmse:.6f}",
                        'AUC': f"{batch_auc:.6f}"
                    })
               
        # --- åˆ†å¸ƒå¼åŒæ­¥ï¼ˆå…³é”®ä¿®æ”¹ï¼‰---
        if dist.is_initialized():
            # åŒæ­¥æŸå¤±å’Œå‡†ç¡®ç‡
            total_loss_tensor = torch.tensor(total_loss).to(self.device)
            total_samples_tensor = torch.tensor(total_samples).to(self.device)
            correct_tensor = torch.tensor(correct_predictions).to(self.device)
            dist.all_reduce(total_loss_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(total_samples_tensor, op=dist.ReduceOp.SUM)
            dist.all_reduce(correct_tensor, op=dist.ReduceOp.SUM)
            total_loss = total_loss_tensor.item()
            total_samples = total_samples_tensor.item()
            correct_predictions = correct_tensor.item()

            # åŒæ­¥æ¦‚ç‡å’Œæ ‡ç­¾
            all_targets_tensor = torch.tensor(np.array(all_targets), dtype=torch.long, device=self.device)
            all_probs_tensor = torch.tensor(np.array(all_probs), device=self.device)
            target_list = [torch.zeros_like(all_targets_tensor) for _ in range(dist.get_world_size())]
            prob_list = [torch.zeros_like(all_probs_tensor) for _ in range(dist.get_world_size())]
            dist.all_gather(target_list, all_targets_tensor)
            dist.all_gather(prob_list, all_probs_tensor)
            all_targets = torch.cat(target_list).cpu().numpy().astype(int)
            all_probs = torch.cat(prob_list).cpu().numpy()

        # --- å…¨å±€æŒ‡æ ‡è®¡ç®— ---
        epoch_loss = total_loss / total_samples if total_samples > 0 else 0.0
        epoch_acc = correct_predictions / total_samples if total_samples > 0 else 0.0
        epoch_rmse = np.sqrt(mean_squared_error(all_targets, all_probs))
        epoch_auc = roc_auc_score(all_targets, all_probs)
        # æ–°å¢F1è®¡ç®—
        all_preds = (np.array(all_probs) >= 0.5).astype(int)
        epoch_f1 = f1_score(all_targets, all_preds)  # äºŒåˆ†ç±»é»˜è®¤average='binary'
        
        if self.rank == 0:
            # è®°å½•åˆ°TensorBoard
            
            self.writer.add_scalar('Epoch/Train_Loss', epoch_loss, epoch)
            self.writer.add_scalar('Epoch/Train_Accuracy', epoch_acc, epoch)
            self.writer.add_scalar('Epoch/Train_RMSE', epoch_rmse, epoch)
            self.writer.add_scalar('Epoch/Train_AUC', epoch_auc, epoch)
            self.writer.add_scalar('Epoch/Train_F1', epoch_f1, epoch)  # æ–°å¢è¡Œ
        
        return epoch_loss, epoch_acc, epoch_rmse, epoch_auc, epoch_f1  # è¿”å›F1
    '''
    def train(self, train_loader, val_loader,test_loader,train_sampler=None, val_sampler=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        start_time = time.time()
        


        
        early_stopper = EarlyStopper(patience=5, min_delta=0.001)
       
        best_test_metrics = {
                'auc': {'value': 0, 'epoch': 0},
                'rmse': {'value': float('inf'), 'epoch': 0},  # RMSE è¶Šå°è¶Šå¥½
                'f1': {'value': 0, 'epoch': 0},
                'acc': {'value': 0, 'epoch': 0}
            }
        
        # åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹ä¹‹å‰ï¼ˆæ¯”å¦‚åœ¨ Trainer ç±»çš„åˆå§‹åŒ–æ–¹æ³•æˆ–è®­ç»ƒå¼€å§‹çš„æ–¹æ³•ä¸­ï¼‰æ·»åŠ ï¼š
        torch.autograd.set_detect_anomaly(True)

        for epoch in range(self.config['total_epochs']):
            # åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶è®¾ç½® sampler çš„ epoch,ä¿è¯å¤šæ˜¾å¡è®­ç»ƒæ—¶ï¼Œæ¯ä¸€è½®æ•°æ®éƒ½èƒ½çœŸéšæœºæ‰“ä¹±
            if dist.is_initialized():
                if train_sampler is not None:
                    train_sampler.set_epoch(epoch)  # è®­ç»ƒé›†éœ€è¦ shuffle
                if val_sampler is not None:
                    val_sampler.set_epoch(epoch)  # éªŒè¯é›†å¦‚æœéœ€è¦ shuffle ä¹Ÿè®¾ç½®
            if epoch == 100 and not self.freeze_model_feat:
                #print("ğŸ”„ Epoch 1 reached: Switching to phase 2 (freeze model_feat)")
                
                # 1âƒ£ï¸ å†»ç»“å‚æ•°
                for name, param in self.model.named_parameters():
                    if name.startswith('module.model_feat.'):
                        param.requires_grad = False

                #print("å‚æ•°å†»ç»“å®Œæ¯•ï¼Œæ£€æµ‹ä¸€ä¸‹requires_gradï¼š")
               
                torch.distributed.barrier()#å¤šæ˜¾å¡åŒæ­¥ (Barrier)
                
                # 2âƒ£ï¸ é‡æ–°æ„å»ºä¼˜åŒ–å™¨
                self._init_optimizer_without_model_feat()
                #print("æ–°ä¼˜åŒ–å™¨å‚æ•°æ•°é‡ï¼š", sum(p.numel() for group in self.optimizer.param_groups for p in group['params']))
                
                self.freeze_model_feat = True
                
                torch.distributed.barrier()



            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc, train_rmse, train_auc,train_f1 = self.train_epoch(train_loader, epoch)

            

            val_metrics = self.validate(val_loader)
        '''
    def train(self, train_loader, val_loader, test_loader, train_sampler=None, val_sampler=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹ (å·²é›†æˆ Phase 1/2 ç­–ç•¥)"""
        start_time = time.time()
        
        early_stopper = EarlyStopper(patience=5, min_delta=0.001)
       
        best_test_metrics = {
                'auc': {'value': 0, 'epoch': 0},
                'rmse': {'value': float('inf'), 'epoch': 0},
                'f1': {'value': 0, 'epoch': 0},
                'acc': {'value': 0, 'epoch': 0}
            }
        
        torch.autograd.set_detect_anomaly(True)

        print("\nğŸ”§ [Manual Fix] å¼ºåˆ¶é‡ç½®å­¦ä¹ ç‡ä¸æƒé‡è¡°å‡...")
        
        TARGET_LR_MODAL = 1e-4 # 5e-5
        TARGET_LR_BASE  = 1e-3  # 1e-3
        TARGET_WD_MODAL = 1e-3
        TARGET_WD_BASE  = 1e-3

        # Group 0: Modal
        if len(self.optimizer.param_groups) > 0:
            self.optimizer.param_groups[0]['lr'] = TARGET_LR_MODAL
            self.optimizer.param_groups[0]['initial_lr'] = TARGET_LR_MODAL
            self.optimizer.param_groups[0]['weight_decay'] = TARGET_WD_MODAL
            print(f"   >>> Group 0 Reset: LR={TARGET_LR_MODAL}, WD={TARGET_WD_MODAL}")

        # Group 1: Base
        if len(self.optimizer.param_groups) > 1:
            self.optimizer.param_groups[1]['lr'] = TARGET_LR_BASE
            self.optimizer.param_groups[1]['initial_lr'] = TARGET_LR_BASE
            self.optimizer.param_groups[1]['weight_decay'] = TARGET_WD_BASE
            print(f"   >>> Group 1 Reset: LR={TARGET_LR_BASE},  WD={TARGET_WD_BASE}")
            
        print("âœ… é‡ç½®å®Œæˆï¼Œå¼€å§‹è®­ç»ƒå¾ªç¯...\n")
        # ==================== è®­ç»ƒä¸»å¾ªç¯ ====================
        for epoch in range(self.config['total_epochs']):
            
            # 1. è®¾ç½® Sampler (å¤šå¡å¿…å¤‡)
            if dist.is_initialized():
                if train_sampler is not None: train_sampler.set_epoch(epoch)
                if val_sampler is not None:   val_sampler.set_epoch(epoch)
            
            # 2. ğŸ”¥ã€å…³é”®ä¿®æ”¹ã€‘è°ƒç”¨åˆ†é˜¶æ®µç­–ç•¥
            # è¿™è¡Œä»£ç ä¼šè‡ªåŠ¨æ ¹æ® epoch å†³å®šé”æ­»å“ªäº›å±‚
            #self._apply_phase_strategy(epoch)
            
            # æ³¨æ„ï¼šä¸éœ€è¦ barrierï¼Œå› ä¸ºæ‰€æœ‰æ˜¾å¡éƒ½ä¼šè¿è¡Œè¿™è¡Œä»£ç 
            
            # 3. è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc, train_rmse, train_auc, train_f1 = self.train_epoch(train_loader, epoch)

            # 4. éªŒè¯é˜¶æ®µ
            val_metrics = self.validate(val_loader)
            # éªŒè¯é˜¶æ®µï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if self.rank == 0:
                
                self.writer.add_scalar('Epoch/Val_AUC', val_metrics['auc'], epoch)
                self.writer.add_scalar('Epoch/Val_RMSE', val_metrics['rmse'], epoch)
                self.writer.add_scalar('Epoch/Val_Accuracy', val_metrics['acc'], epoch)
                self.writer.add_scalar('Epoch/Val_F1', val_metrics['f1'], epoch)
                
                if val_metrics['auc'] > self.best_metric:
                    self.best_metric = val_metrics['auc']
                    #self._save_checkpoint(epoch, val_metrics)

                log_str = (f"Epoch {epoch+1}/{self.config['total_epochs']} | "
                            f"Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train RMSE: {train_rmse:.6f} | Train F1: {train_f1:.6f} | Train AUC: {train_auc:.6f} | "
                            f"Val AUC: {val_metrics['auc']:.6f} | Val RMSE: {val_metrics['rmse']:.6f} | Val F1: {val_metrics['f1']:.6f} |Val Acc: {val_metrics['acc']:.6f}")
                logging.info(log_str)
                print(log_str)
                self.scheduler.step(val_metrics['auc'])
                with open("train_logs_E.txt", "a") as f:
                    f.write(log_str + "\n")

                if early_stopper.should_stop(val_metrics['auc']):
                    print(f"Early stopping at epoch {epoch}")
                    break

            # åŒæ­¥ç‚¹ï¼šæ‰€æœ‰è¿›ç¨‹å¿…é¡»å‚ä¸
            if dist.is_initialized():
                # åˆ›å»ºåŒæ­¥å¼ é‡ï¼ˆå¿…é¡»ç›¸åŒè®¾å¤‡å’Œç±»å‹ï¼‰
                sync_tensor = torch.tensor(0, device=self.device, dtype=torch.int32)
                dist.broadcast(sync_tensor, src=0)  # é˜»å¡ç›´åˆ°æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ­¤ç‚¹

            # è®°å½•æœ€ä½³æµ‹è¯•ç»“æœï¼Œåˆ†åˆ«å­˜å‚¨4ä¸ªæŒ‡æ ‡çš„æœ€ä½³å€¼åŠå…¶å¯¹åº”çš„ epoch
            # ä»…ä¸»è¿›ç¨‹æµ‹è¯•
            test_metrics = self.validate(test_loader)
            if epoch % 1 == 0 and self.rank == 0:
                
                self.writer.add_scalar('Epoch/Test_AUC', test_metrics['auc'], epoch)
                self.writer.add_scalar('Epoch/Test_RMSE', test_metrics['rmse'], epoch)
                self.writer.add_scalar('Epoch/Test_Accuracy', test_metrics['acc'], epoch)
                self.writer.add_scalar('Epoch/Test_F1', test_metrics['f1'], epoch)

                log_str = (f"Epoch {epoch+1}/{self.config['total_epochs']} | "
                            f"Test AUC: {test_metrics['auc']:.6f} | Test RMSE: {test_metrics['rmse']:.6f} | "
                            f"Test F1: {test_metrics['f1']:.6f} | Test Acc: {test_metrics['acc']:.6f}")
                
                logging.info(log_str)
                print(log_str)
                with open("train_logs_E.txt", "a") as f:
                    f.write(log_str + "\n")

                # è®°å½•æœ€ä½³æµ‹è¯•ç»“æœï¼ˆåˆ†åˆ«åˆ¤æ–­æ¯ä¸ªæŒ‡æ ‡æ˜¯å¦æ›´ä¼˜ï¼‰
                if test_metrics['auc'] > best_test_metrics['auc']['value']:
                    best_test_metrics['auc']['value'] = test_metrics['auc']
                    best_test_metrics['auc']['epoch'] = epoch + 1

                if test_metrics['rmse'] < best_test_metrics['rmse']['value']:  # RMSE è¶Šå°è¶Šå¥½
                    best_test_metrics['rmse']['value'] = test_metrics['rmse']
                    best_test_metrics['rmse']['epoch'] = epoch + 1

                if test_metrics['f1'] > best_test_metrics['f1']['value']:
                    best_test_metrics['f1']['value'] = test_metrics['f1']
                    best_test_metrics['f1']['epoch'] = epoch + 1

                if test_metrics['acc'] > best_test_metrics['acc']['value']:
                    best_test_metrics['acc']['value'] = test_metrics['acc']
                    best_test_metrics['acc']['epoch'] = epoch + 1

            # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½åŒæ­¥
            if dist.is_initialized():
                dist.broadcast(torch.tensor([0], device=self.device), src=0)

            # åœ¨è®­ç»ƒå¾ªç¯ç»“æŸåæ·»åŠ 
        if dist.is_initialized():
            dist.destroy_process_group()  # å¿…é¡»çš„æ¸…ç†æ“ä½œ 

            # ä¸»è¿›ç¨‹ä¿å­˜æœ€ç»ˆæ¨¡å‹å¹¶æ‰“å°æœ€ä½³ç»“æœ
        if self.rank == 0:
            #self._save_checkpoint(epoch, val_metrics, final=True)

            # Get current learning rate from the optimizer
            current_lr = self.optimizer.param_groups[0]['lr']
            init_lr = self.config['learning_rate_1']  # ğŸ’¡ åŠ ä¸Šè¿™ä¸€å¥è·å–åˆå§‹å­¦ä¹ ç‡

            best_log_str = (f"\nBest Test Results:\n"
                            f"- Best AUC: {best_test_metrics['auc']['value']:.6f} (Epoch {best_test_metrics['auc']['epoch']})\n"
                            f"- Best RMSE: {best_test_metrics['rmse']['value']:.6f} (Epoch {best_test_metrics['rmse']['epoch']})\n"
                            f"- Best F1: {best_test_metrics['f1']['value']:.6f} (Epoch {best_test_metrics['f1']['epoch']})\n"
                            f"- Best Acc: {best_test_metrics['acc']['value']:.6f} (Epoch {best_test_metrics['acc']['epoch']})\n"
                            f"- Initial Learning Rate: {init_lr:.8f}\n"      # âœ¨ æ–°å¢è¿™ä¸€è¡Œ
                            f"- Current Learning Rate: {current_lr:.8f}\n") # åŸæ¥è¿™ä¸€è¡Œä¿ç•™

            print(best_log_str)
            logging.info(best_log_str)
            with open("train_logs_E.txt", "a") as f:
                f.write(best_log_str)

            print(f"è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")

           
       
    
    def _save_checkpoint(self, epoch, metrics, final=False):
        """æ¨¡å‹ä¿å­˜ï¼ˆå«å…ƒæ•°æ®ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state': self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'metrics': metrics,
            'config': self.config
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ncdm_{'final' if final else 'best'}_{timestamp}_auc{metrics['auc']:.6f}.pt"
        save_path = os.path.join(self.config['model_dir'], filename)
        
        torch.save(checkpoint, save_path)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

    def print_gpu(self,tag=""):
        allocated = torch.cuda.memory_allocated() / 1024 / 1024
        reserved = torch.cuda.memory_reserved() / 1024 / 1024
        print(f"[{tag}] Allocated: {allocated:.2f} MB | Reserved: {reserved:.2f} MB")
    def validate(self, val_loader):
        self.model.eval()
        all_targets = []
        all_probs = []

        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(f'cuda:{self.rank}') for k, v in batch.items()}
                with autocast(device_type='cuda'):
                    output_1 ,_,_,_= self.model.forward(
                       batch
                    ) 
                    output_1 =output_1.squeeze()

                    # å…³é”®ä¿®æ”¹1ï¼šæ˜¾å¼äºŒå€¼åŒ–æ ‡ç­¾
                    targets = (batch['corrects'].squeeze().float() >= 0.5).float()
                    probs = torch.sigmoid(output_1)
                    if torch.isnan(output_1).any():
                        print("ğŸ”¥ output_1 é‡Œå‡ºç° NaN")
                    if torch.isnan(probs).any():
                        print("ğŸ”¥ probs é‡Œå‡ºç° NaN")

                    
                all_targets.extend(targets.cpu().numpy().flatten())
                all_probs.extend(probs.detach().cpu().numpy().flatten())

        # --- åˆ†å¸ƒå¼åŒæ­¥ ---
        if dist.is_initialized():
            all_targets_tensor = torch.tensor(np.array(all_targets), dtype=torch.float, device=self.device)
            all_probs_tensor = torch.tensor(np.array(all_probs), device=self.device)
            target_list = [torch.zeros_like(all_targets_tensor) for _ in range(dist.get_world_size())]
            prob_list = [torch.zeros_like(all_probs_tensor) for _ in range(dist.get_world_size())]
            dist.all_gather(target_list, all_targets_tensor)
            dist.all_gather(prob_list, all_probs_tensor)
            all_targets = torch.cat(target_list).cpu().numpy()
            all_probs = torch.cat(prob_list).cpu().numpy()

        # å…³é”®ä¿®æ”¹2ï¼šåŒæ­¥åå†æ¬¡äºŒå€¼åŒ–
        all_targets = np.array(all_targets)  # è½¬æ¢åˆ—è¡¨ä¸ºæ•°ç»„
        all_targets = np.where(all_targets >= 0.5, 1.0, 0.0).astype(np.float32)

            # å…³é”®ä¿®æ”¹3ï¼šç¡®ä¿ all_probs æ˜¯æ•°ç»„
        # --- ç¡®ä¿ all_probs æ˜¯æ•°ç»„ ---
        all_probs = np.array(all_probs)  # è½¬æ¢åˆ—è¡¨ä¸ºæ•°ç»„

        # --- ä¿®æ­£ï¼šç¡®ä¿ all_probs åœ¨ [0, 1] ä¹‹é—´ ---
        all_probs = np.clip(all_probs, 0.0, 1.0)

        # --- åˆæ³•æ€§æ£€æŸ¥ ---
        assert np.isin(all_targets, [0, 1]).all(), f"éæ³•æ ‡ç­¾å€¼: {np.unique(all_targets)}"
        assert (all_probs >= 0).all() and (all_probs <= 1).all(), f"æ¦‚ç‡å€¼è¶…å‡ºèŒƒå›´: {np.unique(all_probs)}"


        # --- æŒ‡æ ‡è®¡ç®— ---
        # --- æŒ‡æ ‡è®¡ç®— ---
        total_samples = len(all_targets)
        all_preds = (all_probs >= 0.5).astype(int)  # æ–°å¢é¢„æµ‹æ ‡ç­¾ç”Ÿæˆ
        correct_predictions = (all_targets == all_preds).sum()
        
        epoch_acc = correct_predictions / total_samples
        epoch_auc = roc_auc_score(all_targets, all_probs)
        epoch_rmse = np.sqrt(mean_squared_error(all_targets, all_probs))
        epoch_f1 = f1_score(all_targets, all_preds)  # æ–°å¢F1è®¡ç®—

        return {
            'acc': epoch_acc, 
            'auc': epoch_auc, 
            'rmse': epoch_rmse,
            'f1': epoch_f1  # æ–°å¢è¿”å›é¡¹
        }
    '''
    def comprehensive_gradient_analysis(self, model, scaler):
        # åˆ†å¸ƒå¼æ£€æŸ¥ï¼šåªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œï¼ŒèŠ‚çœå…¶ä»–æ˜¾å¡çš„èµ„æº
        if dist.is_initialized() and dist.get_rank() != 0:
            return

        # ================= é…ç½®åŒº =================
        THRESHOLD_WEIGHT_SMALL = 1e-6   
        THRESHOLD_BIAS_SMALL = 1e-9    
        
        print("\n" + "="*80)
        print("ğŸ”¬ æ™ºèƒ½æ¢¯åº¦å¥åº·æ£€æŸ¥æŠ¥å‘Š (DDP å…¼å®¹ç‰ˆ)")
        print("="*80)
        
        total_norm = 0.0
        vanished_weights = 0
        total_weights = 0
        total_biases = 0
        has_nan = False
        
        module_stats = {}

        for name, p in model.named_parameters():
            if p.grad is None:
                continue

            # ã€æ–°å¢ã€‘å…³é”®å®‰å…¨æ£€æŸ¥ï¼šæ£€æµ‹ NaN å’Œ Inf
            # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå› ä¸ºä¸€æ—¦å‡ºç° NaNï¼Œåç»­çš„ norm è®¡ç®—éƒ½ä¼šå¤±æ•ˆ
            if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                has_nan = True
                print(f"ğŸ’€ [è‡´å‘½é”™è¯¯] åœ¨å±‚ {name} ä¸­å‘ç° NaN/Infï¼")
                # é‡åˆ° NaN é€šå¸¸å¯ä»¥ç›´æ¥è·³è¿‡ç»Ÿè®¡ï¼Œæˆ–è€…è®°å½•ä¸‹æ¥
                continue

            grad_norm = p.grad.detach().norm().item()
            total_norm += grad_norm ** 2
            
            # åŒºåˆ† Bias å’Œ Weight
            is_bias = 'bias' in name.lower() or 'norm' in name.lower()
            
            if is_bias:
                total_biases += 1
                if grad_norm < THRESHOLD_BIAS_SMALL:
                    # Bias æ¶ˆå¤±é€šå¸¸ä¸éœ€è¦æŠ¥è­¦
                    pass
            else:
                total_weights += 1
                if grad_norm < THRESHOLD_WEIGHT_SMALL:
                    vanished_weights += 1
                    # é™åˆ¶æ‰“å°æ•°é‡ï¼Œé˜²æ­¢åˆ·å±
                    if vanished_weights <= 5:
                        print(f"ğŸ”´ [æƒé‡æ­»å¯‚] {name}: {grad_norm:.2e}")

            # =====================================================
            # ã€å…³é”®ä¿®å¤ã€‘ DDP å‘½åå‰ç¼€å¤„ç†
            # =====================================================
            # å¦‚æœæ˜¯ DDPï¼Œåå­—æ˜¯ "module.backbone.0..."
            # æˆ‘ä»¬éœ€è¦å»æ‰ "module." æ‰èƒ½æå–çœŸæ­£çš„æ¨¡å—å "backbone"
            clean_name = name
            if clean_name.startswith('module.'):
                clean_name = clean_name[7:] # å»æ‰å‰7ä¸ªå­—ç¬¦ "module."
            
            # æå–ç¬¬ä¸€çº§æ¨¡å—å
            module_name = clean_name.split('.')[0]
            
            if module_name not in module_stats:
                module_stats[module_name] = {'grad_sum': 0.0, 'count': 0, 'max': 0.0}
            
            module_stats[module_name]['grad_sum'] += grad_norm
            module_stats[module_name]['count'] += 1
            module_stats[module_name]['max'] = max(module_stats[module_name]['max'], grad_norm)

        total_norm = total_norm ** 0.5
        
        # ================= è¾“å‡ºæ‘˜è¦ =================
        print("-" * 80)
        print(f"ğŸ“Š æ•´ä½“å¥åº·åº¦æ‘˜è¦:")
        
        if has_nan:
            print(f" Â  ğŸ’€ çŠ¶æ€: ã€å±é™©ã€‘æ£€æµ‹åˆ° NaN æˆ– Infï¼Œæ¨¡å‹å¯èƒ½å·²å‘æ•£ï¼")
        else:
            print(f" Â  â¤ æ€»æ¢¯åº¦èŒƒæ•°: {total_norm:.4f}")
        
        print(f" Â  â¤ å½“å‰ Scale: {scaler.get_scale()}")
        
        w_vanish_rate = (vanished_weights/total_weights*100) if total_weights > 0 else 0
        print(f" Â  â¤ æƒé‡å±‚æ´»è·ƒåº¦: {total_weights - vanished_weights}/{total_weights} (æ¶ˆå¤±ç‡: {w_vanish_rate:.1f}%)")
        
        if w_vanish_rate > 20:
             print(f" Â  Â  Â ğŸ”´ è­¦å‘Šï¼šå¤§é‡æƒé‡åœæ­¢æ›´æ–°ï¼")

        # ================= æ¨¡å—é€è§† =================
        print("\nğŸ› ï¸ Â å„æ¨¡å—â€œå‡ºåŠ›â€æƒ…å†µ (å¹³å‡æ¢¯åº¦):")
        print(f" Â  {'æ¨¡å—å':<25} | {'å¹³å‡æ¢¯åº¦':<12} | {'æœ€å¤§æ¢¯åº¦':<12} | {'çŠ¶æ€'}")
        print("-" * 80)
        
        # æ’åºè¾“å‡ºï¼Œæ–¹ä¾¿æŸ¥çœ‹
        for name in sorted(module_stats.keys()):
            stats = module_stats[name]
            avg_grad = stats['grad_sum'] / stats['count']
            
            status = ""
            if avg_grad > 1.0: status = "ğŸ’£ å¯èƒ½çˆ†ç‚¸"
            elif avg_grad > 1e-2: status = "ğŸ”¥ å‰§çƒˆæ›´æ–°"
            elif avg_grad > 1e-3: status = "âœ… ç¨³æ­¥æ›´æ–°"
            elif avg_grad > 1e-5: status = "ğŸ’¤ å¾®è°ƒä¸­"
            else: status = "â„ï¸ å‡ ä¹å†»ç»“"
            
            print(f" Â  {name:<25} | {avg_grad:.2e} Â  Â  | {stats['max']:.2e} Â  Â  | {status}")

        print("="*80 + "\n")

    '''
    def comprehensive_gradient_analysis(self, model, scaler):
        if dist.is_initialized() and dist.get_rank() != 0:
            return

        print("\n" + "="*80)
        print("ğŸ”¬ æ¢¯åº¦å¥åº·æ£€æŸ¥ - æ‰€æœ‰å‚æ•°")
        print("="*80)
        
        # è§£åŒ…æ¨¡å‹
        if hasattr(model, 'module'):
            real_model = model.module
        else:
            real_model = model
        
        if hasattr(real_model, '_orig_mod'):
            real_model = real_model._orig_mod
        
        # é¦–å…ˆæ£€æŸ¥å†»ç»“å‚æ•°
        frozen_params = []
        trainable_params = []
        
        for name, param in real_model.named_parameters():
            if param.requires_grad:
                trainable_params.append(name)
            else:
                frozen_params.append(name)
        
        # è¾“å‡ºå†»ç»“å‚æ•°ä¿¡æ¯
        if frozen_params:
            print("ğŸ§Š å†»ç»“å‚æ•°ç»Ÿè®¡:")
            print(f"  å…±æœ‰ {len(frozen_params)} ä¸ªå‚æ•°è¢«å†»ç»“ (requires_grad=False)")
            
            # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡å†»ç»“å‚æ•°
            frozen_by_module = {}
            for name in frozen_params:
                parts = name.split('.')
                module_name = parts[0] if parts else 'unknown'
                if len(parts) > 1:
                    module_name = f"{parts[0]}.{parts[1]}"
                
                if module_name not in frozen_by_module:
                    frozen_by_module[module_name] = []
                frozen_by_module[module_name].append(name)
            
            print("\nğŸ“Œ æŒ‰æ¨¡å—å†»ç»“å‚æ•°ç»Ÿè®¡:")
            for module, params in frozen_by_module.items():
                print(f"  ğŸ“ {module}: {len(params)} ä¸ªå†»ç»“å‚æ•°")
                # æ˜¾ç¤ºå‰3ä¸ªå†»ç»“å‚æ•°ä½œä¸ºç¤ºä¾‹
                for i, param_name in enumerate(params[:3]):
                    print(f"     {i+1}. {param_name}")
                if len(params) > 3:
                    print(f"     ... è¿˜æœ‰ {len(params)-3} ä¸ªå‚æ•°")
            
            print("-"*80 + "\n")
        
        # åªå¯¹å¯è®­ç»ƒå‚æ•°è¿›è¡Œæ¢¯åº¦åˆ†æ
        print("ğŸ“‹ å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦åˆ†æ:")
        print("-"*100)
        
        # æ”¶é›†æ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„æ¢¯åº¦ä¿¡æ¯
        grad_info = []
        has_nan = False
        total_norm = 0.0
        
        for name, param in real_model.named_parameters():
            if not param.requires_grad:
                continue  # è·³è¿‡å†»ç»“å‚æ•°
                
            if param.grad is None:
                grad_norm = 0.0
                has_grad = False
            else:
                grad = param.grad
                if torch.isnan(grad).any() or torch.isinf(grad).any():
                    has_nan = True
                    grad_norm = float('nan')
                    has_grad = True
                else:
                    grad_norm = grad.norm().item()
                    total_norm += grad_norm ** 2
                    has_grad = True
            
            # å­˜å‚¨ä¿¡æ¯
            grad_info.append({
                'name': name,
                'grad_norm': grad_norm,
                'has_grad': has_grad,
                'has_nan': torch.isnan(grad).any() if has_grad else False,
                'has_inf': torch.isinf(grad).any() if has_grad else False
            })
            
            # æ˜¾ç¤ºæ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
            if has_grad and not (torch.isnan(grad).any() or torch.isinf(grad).any()):
                # æŒ‰æ¢¯åº¦å¤§å°æ’åºæ˜¾ç¤º
                status = ""
                if grad_norm > 1.0: status = "ğŸ’£"
                elif grad_norm > 0.1: status = "ğŸ”¥"
                elif grad_norm > 0.01: status = "âœ…"
                elif grad_norm > 0.001: status = "ğŸ’¤"
                else: status = "â„ï¸"
                
                print(f"{status} {name}: grad_norm={grad_norm:.6f}")
        
        # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
        total_norm = total_norm ** 0.5 if total_norm > 0 else 0.0
        
        # æ’åºæ˜¾ç¤º
        if grad_info:
            print("\nğŸ“Š æ¢¯åº¦æœ€å¤§å‰10ä¸ªå‚æ•°:")
            sorted_grads = sorted([g for g in grad_info if g['has_grad'] and not g['has_nan'] and not g['has_inf']], 
                                key=lambda x: x['grad_norm'], reverse=True)
            
            for i, g in enumerate(sorted_grads[:10]):
                print(f"{i+1:2d}. {g['name']}: {g['grad_norm']:.6f}")
            
            print("\nğŸ“‰ æ¢¯åº¦æœ€å°å‰10ä¸ªå‚æ•°:")
            sorted_grads_small = sorted([g for g in grad_info if g['has_grad'] and not g['has_nan'] and not g['has_inf']], 
                                    key=lambda x: x['grad_norm'])
            
            for i, g in enumerate(sorted_grads_small[:10]):
                print(f"{i+1:2d}. {g['name']}: {g['grad_norm']:.6f}")
        
        print("\n" + "-"*80)
        print(f"ğŸ“ˆ æ±‡æ€»:")
        print(f"   æ€»å¯è®­ç»ƒå‚æ•°: {len(trainable_params)}")
        if frozen_params:
            print(f"   å†»ç»“å‚æ•°: {len(frozen_params)}")
        print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_norm:.6f}")
        if grad_info:
            print(f"   æœ‰æ¢¯åº¦å‚æ•°: {sum(1 for g in grad_info if g['has_grad'])}")
            print(f"   é›¶æ¢¯åº¦å‚æ•°: {sum(1 for g in grad_info if not g['has_grad'])}")
        if has_nan:
            print(f"   ğŸ’€ æ£€æµ‹åˆ°NaN/Infæ¢¯åº¦!")
        print("="*80 + "\n")

# é…ç½®å›¾åƒé¢„å¤„ç†ï¼ˆä¸ProblemDatasetä¸€è‡´ï¼‰
image_transform = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)
])
torch.autograd.set_detect_anomaly(True)  # åŠ å…¥è¿™è¡Œï¼è¿è¡Œåä¼šæ˜¾ç¤ºå…·ä½“æŠ¥é”™ä½ç½®


import torch.distributed as dist
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader
import os
import os
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
from tqdm import tqdm
import json
from filelock import FileLock
import os
def save_fused_features(model, dataset, batch_size=4):
    """
    ä¿å­˜èåˆç‰¹å¾ã€æ–‡æœ¬ç‰¹å¾å’Œå›¾åƒç‰¹å¾ä¸º .pt æ–‡ä»¶
    :param model: æ¨¡å‹
    :param dataset: æ•°æ®é›†
    :param output_dir: è¾“å‡ºç›®å½•
    :param batch_size: æ‰¹å¤§å°
    """
    # ç¡®ä¿è¿›ç¨‹ç»„å·²åˆå§‹åŒ–
    if not dist.is_initialized():
        dist.init_process_group(
            backend="nccl" if torch.cuda.is_available() else "gloo",
            init_method="env://"
        )
    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # å¦‚æœæ˜¯ DDP åŒ…è£…çš„æ¨¡å‹ï¼Œè·å–åŸå§‹æ¨¡å‹
    if isinstance(model, torch.nn.parallel.DistributedDataParallel):
        model = model.module

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # æ•°æ®åŠ è½½å™¨
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=8,
        collate_fn=lambda batch: {
            'pids': [item['pid'] for item in batch],
            'image': torch.stack([item['image'] for item in batch]),
            'text': [item['text'] for item in batch]
        }
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰
    if rank == 0:
        os.makedirs(TEXT_FEATURES_DIR, exist_ok=True)
        os.makedirs(IMAGE_FEATURES_DIR, exist_ok=True)
        os.makedirs(FUSION_FEATURES_PATH, exist_ok=True)
    dist.barrier()  # åŒæ­¥æ‰€æœ‰è¿›ç¨‹



    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        gate_dict = {}

        for batch in tqdm(loader, desc="ç‰¹å¾æå–ä¸ä¿å­˜", disable=rank != 0):
            batch = {
                'pid': batch['pids'],
                'image': batch['image'].to(device),
                'text': batch['text']
            }

            text_feat = model.text_feature.process_batch(
                list(zip(batch['pid'], batch['text']))
            )
            text_feat = [x.to(device) for x in text_feat]
            #img_feats, img_feat = model.img_feature(batch['image'].float().to(device))
            #fused_feat = model.extract_features(batch)
            

            for i, pid in enumerate(batch['pid']):
                '''
                # ======================= èåˆç‰¹å¾ä¿å­˜ =======================
                file_path = FUSION_FEATURES_PATH / f"{pid}.pt"
                lock_path = str(file_path) + ".lock"
                with FileLock(lock_path):
                    if file_path.exists():
                        #print(f"æ–‡ä»¶å·²å­˜åœ¨: {file_path}")
                        os.remove(file_path)
                    torch.save(fused_feat[i].cpu(), file_path)
                '''
                 # ======================= æ–‡æœ¬ç‰¹å¾ä¿å­˜ï¼ˆåªä¿å­˜æœ€åä¸€å±‚ï¼‰ =======================
                file_path = TEXT_FEATURES_DIR / f"{pid}.pt"
                lock_path = str(file_path) + ".lock"
                with FileLock(lock_path):
                    if file_path.exists():
                        os.remove(file_path)
                    torch.save(text_feat[-1][i].cpu(), file_path)  # è¿™é‡Œtext_feat[-1]æ˜¯æœ€åä¸€å±‚ï¼Œç´¢å¼•iå–å¯¹åº”æ ·æœ¬
                '''
                # ======================= å›¾åƒç‰¹å¾ä¿å­˜ =======================
                file_path = IMAGE_FEATURES_DIR / f"{pid}.pt"
                lock_path = str(file_path) + ".lock"
                with FileLock(lock_path):
                    if file_path.exists():
                        print(f"æ–‡ä»¶å·²å­˜åœ¨: {file_path}")
                        os.remove(file_path)
                    torch.save(img_feat[i].cpu(), file_path)
                '''

        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        dist.barrier()
def save_Img_Text_features(model, dataset, batch_size=4):
    """
    ä¿å­˜èåˆç‰¹å¾ã€æ–‡æœ¬ç‰¹å¾å’Œå›¾åƒç‰¹å¾ä¸º .pt æ–‡ä»¶
    :param model: æ¨¡å‹
    :param dataset: æ•°æ®é›†
    :param output_dir: è¾“å‡ºç›®å½•
    :param batch_size: æ‰¹å¤§å°
    """
    # ç¡®ä¿è¿›ç¨‹ç»„å·²åˆå§‹åŒ–
    if not dist.is_initialized():
        dist.init_process_group(
            backend="nccl" if torch.cuda.is_available() else "gloo",
            init_method="env://"
        )
    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # å¦‚æœæ˜¯ DDP åŒ…è£…çš„æ¨¡å‹ï¼Œè·å–åŸå§‹æ¨¡å‹
    if isinstance(model, torch.nn.parallel.DistributedDataParallel):
        model = model.module

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # æ•°æ®åŠ è½½å™¨
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=8,
        collate_fn=lambda batch: {
            'pids': [item['pid'] for item in batch],
            'image': torch.stack([item['image'] for item in batch]),
            'text': [item['text'] for item in batch]
        }
    )

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        gate_dict = {}

        for batch in tqdm(loader, desc="ç‰¹å¾æå–ä¸ä¿å­˜", disable=rank != 0):
            batch = {
                'pid': batch['pids'],
                'image': batch['image'].to(device),
                'text': batch['text']
            }

            model.text_feature.save_features(list(zip(batch['pid'], batch['text'])))
            
            model.img_feature.save_features_from_images(batch['image'], batch['pid'])
            
        # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
        dist.barrier()


import torch.distributed as dist

def validate_device_consistency(data, model):
    current_device = torch.cuda.current_device()
    # æ£€æŸ¥æ•°æ®è®¾å¤‡
    for k, v in data.items():
        if isinstance(v, torch.Tensor):
            assert v.device == torch.device(f'cuda:{current_device}'), \
                f"æ•°æ® {k} è®¾å¤‡ä¸ä¸€è‡´: {v.device} vs cuda:{current_device}"
    # æ£€æŸ¥æ¨¡å‹è®¾å¤‡
    for param in model.parameters():
        assert param.device == torch.device(f'cuda:{current_device}'), \
            f"æ¨¡å‹å‚æ•°è®¾å¤‡ä¸ä¸€è‡´: {param.device}"
        

import os
import torch
import torch.distributed as dist
from datetime import timedelta
import argparse
from torch.utils.data import DataLoader, DistributedSampler
# åœ¨ __main__ å—æœ€å‰é¢æ·»åŠ 
import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  # å¼ºåˆ¶ä¸nvidia-smiè®¾å¤‡é¡ºåºä¸€è‡´ 
os.environ["NCCL_IB_DISABLE"] = "1"             # ç¦ç”¨InfiniBandï¼ˆå› æ—¥å¿—æ˜¾ç¤ºmlx5è®¾å¤‡æœªæ‰¾åˆ°ï¼‰

def setup_distributed():
    # è‡ªåŠ¨æ£€æµ‹è¿è¡Œæ¨¡å¼
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    rank = int(os.environ.get("RANK", 0))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    
    # å•å¡æ¨¡å¼ç›´æ¥è¿”å›
    if world_size == 1 or not torch.cuda.is_available():
        return local_rank, world_size  # è¿”å›local_rankè€Œä¸æ˜¯å…¨å±€rank
    
    # å¤šå¡åˆå§‹åŒ–æµç¨‹
    if not dist.is_initialized():
        backend = "nccl" if torch.cuda.is_available() else "gloo"
        dist.init_process_group(
            backend=backend,
            init_method="env://",
            timeout=timedelta(seconds=180),
            world_size=world_size,
            rank=rank
        )
    
    # æ˜¾å¼è®¾å¤‡ç»‘å®šï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
    torch.cuda.set_device(local_rank)
    # éªŒè¯è®¾å¤‡ç»‘å®š
    assert torch.cuda.current_device() == local_rank, \
        f"Device binding failed! Expected {local_rank}, got {torch.cuda.current_device()}"
    return local_rank, world_size

def parse_args():
    parser = argparse.ArgumentParser()
    # å¿…é¡»ä¿ç•™çš„å‚æ•°ï¼ˆtorchrunä¼šè‡ªåŠ¨æ³¨å…¥ï¼‰
    parser.add_argument("--local_rank", type=int, default=os.environ.get("LOCAL_RANK", 0))
    # å…¶ä»–å‚æ•°...
    return parser.parse_args()
from EndToEndContrastiveModel import EndToEndContrastiveModel
if __name__ == "__main__":

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    args = parse_args()
    local_rank, world_size = setup_distributed()
    torch.set_float32_matmul_precision('high')

   


          

    
    
    # è°ƒè¯•ä¿¡æ¯
    print(f"[Process {os.getpid()}] "
          f"Local Rank: {args.local_rank}, "
          f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')}, "
          f"Current Device: {torch.cuda.current_device()}")
    
    # è®¾å¤‡åˆå§‹åŒ–
    device = torch.device(f'cuda:{args.local_rank}' 
                         if torch.cuda.is_available() else 'cpu')
    
    torch.cuda.set_device(args.local_rank)  # æ˜¾å¼å¼ºåˆ¶ç»‘å®šè®¾å¤‡
   

    # æ‰“å°è®¾å¤‡ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    print(f"Rank {local_rank} å½“å‰GPU: {torch.cuda.current_device()}")
    # åˆå§‹åŒ–æ•°æ®é›†
    train_dataset = RecordDataset(mode='train',rank=args.local_rank)
    val_dataset = RecordDataset(mode='val',rank=args.local_rank)
    test_dataset = RecordDataset(mode='test',rank=args.local_rank)
    

    print("1")
    #problem_dataset = ProblemDataset(transform=image_transform)
    print("2")
    # åˆå§‹åŒ–æ¨¡å‹
    model = Net(
        student_n=train_dataset.user_n,
        exer_n=len(train_dataset.problem_data.valid_pids),
        knowledge_n=TOTAL_SKILLS,
        problem_dataset=train_dataset.problem_data
    ).to(device)
    
    '''
    print(f"Rank {args.local_rank}: Compiling model...")
    try:
        model = torch.compile(model, mode='default')
    except Exception as e:
        print(f"ç¼–è¯‘å¤±è´¥ï¼Œå›é€€: {e}")
    '''
    '''
    print(f"Rank {args.local_rank}: Compiling model...")
    try:
        # å¯ç”¨åŠ¨æ€å½¢çŠ¶æ”¯æŒ
        torch._dynamo.config.automatic_dynamic_shapes = True
        torch._dynamo.config.assume_static_by_default = False
        
        # ç¼–è¯‘æ¨¡å‹ï¼Œå…è®¸åŠ¨æ€å½¢çŠ¶
        model = torch.compile(
            model, 
            mode='default',
            dynamic=True,  # å¯ç”¨åŠ¨æ€å½¢çŠ¶
            fullgraph=False,
        )
        print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸï¼ˆåŠ¨æ€å½¢çŠ¶æ¨¡å¼ï¼‰")
    except Exception as e:
        print(f"âš ï¸ ç¼–è¯‘å¤±è´¥ï¼Œå›é€€åˆ°æœªç¼–è¯‘æ¨¡å¼: {e}")
    '''
    
    try:
        # å¼ºåˆ¶é‡ç½®
        torch._dynamo.reset()
        
        model = torch.compile(
            model,
            mode="reduce-overhead", # æˆ–è€… default
            dynamic=True,
            backend="inductor",
            # å…³é”®åœ¨è¿™é‡Œï¼šä¼ å…¥ options å­—å…¸
            options={
                "shape_padding": True,  # å¼ºåˆ¶å¡«å……åŠ¨æ€ç»´åº¦ï¼Œé¿å… symbolicè®¡ç®—é”™è¯¯
                "triton.cudagraphs": False, # DDP ä¸‹ cudagraphs ææ˜“æŠ¥é”™ï¼Œå»ºè®®å…³é—­
            }
        )
        print("âœ… æ¨¡å‹ç¼–è¯‘æˆåŠŸ (å¼€å¯ Shape Padding)")
    except Exception as e:
        print(f"ç¼–è¯‘å¤±è´¥: {e}")
    print("3")
    # åˆå§‹åŒ–é…ç½®
    config = {
        'total_epochs': 100,
        'learning_rate_1': 0.0001,
        'learning_rate_2': 0.0001,
        'weight_decay': 0.01,
        'grad_clip': 0.5,
        'log_interval': 50,
        'model_dir': '/mnt/proj/autodl-tmp/checkpoints',
        'use_amp': True
    }

    print("4")

    trainer = Trainer(config, model, rank=local_rank)
    print("5")
    def count_parameters(model):
        """
        è®¡ç®—å¹¶æ‰“å° NCDM åŸºç¡€å±‚å’Œ Fusion å±‚çš„å‚æ•°é‡åˆ†å‰²ã€‚
        :param model: ä½ çš„ä¸»æ¨¡å‹å®ä¾‹ã€‚
        :param user_params: ä»ä½ çš„ä»£ç ä¸­è·å–çš„é…ç½®å‚æ•° (ä¾‹å¦‚å­¦ç”Ÿæ•°ï¼ŒçŸ¥è¯†ç‚¹æ•°)ã€‚
        """
        if not dist.is_initialized() or dist.get_rank() == 0:
            total_trainable = 0
            ncdm_core_sum = 0
            fusion_system_sum = 0
            
            # è¯†åˆ«å…³é”®å‚æ•°ç»„
            NCDM_CORE_PREFIXES = ('student_emb', 'k_difficulty_NCDM', 'e_discrimination_NCDM', 'output_layer','W_p', 'diff_head_k', 'know_pro')
            FUSION_CORE_PREFIXES = ('model_feat')
            
            for name, param in model.named_parameters():
                if not param.requires_grad:
                    continue

                param_count = param.numel()
                total_trainable += param_count
                
                # æ£€æŸ¥æ˜¯å¦å±äº NCDM åŸºç¡€å±‚
                if name.startswith(NCDM_CORE_PREFIXES):
                    ncdm_core_sum += param_count
                # æ£€æŸ¥æ˜¯å¦å±äº Fusion/Attention å±‚ (model_feat æ˜¯ä½ çš„èåˆä¸»æ¨¡å—)
                elif name.startswith(FUSION_CORE_PREFIXES):
                    fusion_system_sum += param_count
                else:
                    # å‰©ä¸‹çš„å‚æ•°ï¼Œé€šå¸¸æ˜¯ DDP çš„åŒ…è£…æˆ–æœªå‘½åçš„å‚æ•°
                    pass

            # æ‰“å°è¯¦ç»†åˆ†å‰²æŠ¥å‘Š
            print("\n" + "="*80)
            print("ğŸ’¡ æ ¸å¿ƒæ¨¡å—å‚æ•°é‡åˆ†å‰²æŠ¥å‘Š (å¯è®­ç»ƒéƒ¨åˆ†)")
            print("="*80)
            print(f"æ€»å¯è®­ç»ƒå‚æ•° (Total Trainable): {total_trainable:,}")
            print("-" * 80)
            print(f"1. NCDM åŸºç¡€å‚æ•° (Embeddings/Heads): {ncdm_core_sum:,}")
            print(f"2. Fusion/Attention ç³»ç»Ÿ (model_feat): {fusion_system_sum:,}")
            print(f"3. å‰©ä½™å‚æ•° (å¦‚DDPåŒ…è£…/æœªè¯†åˆ«): {total_trainable - ncdm_core_sum - fusion_system_sum:,}")
            print("-" * 80)
            print(f"   => Fusion ç³»ç»Ÿå æ€»å¯è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹: {fusion_system_sum / total_trainable * 100:.2f}%")
            print("="*80)

    # åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨
    count_parameters(model)
    print("6")
    '''
    train_dataset.records = train_dataset.records[:3000]
    val_dataset.records = val_dataset.records[:3000]
    test_dataset.records = test_dataset.records[:3000]
    '''
    

    '''
    batch_size = 1024
    max_problems = 557

    train_sampler = DistributedBalancedProblemBatchSampler(
        train_dataset,
        batch_size=batch_size,
        max_problems=max_problems,
        num_replicas=world_size,
        rank=local_rank,
        seed=42
    )
    val_sampler = DistributedBalancedProblemBatchSampler(
        val_dataset,
        batch_size=batch_size,
        max_problems=max_problems,
        num_replicas=world_size,
        rank=local_rank,
        seed=42
    )
    test_sampler = DistributedBalancedProblemBatchSampler(
        test_dataset,
        batch_size=batch_size,
        max_problems=max_problems,
        num_replicas=world_size,
        rank=local_rank,
        seed=42
    )
    '''
    # æ•°æ®åŠ è½½å™¨é…ç½®ï¼ˆåˆ†å¸ƒå¼ï¼‰
    if dist.is_initialized():
        train_sampler = DistributedSampler(train_dataset, shuffle=True, num_replicas=world_size, rank=local_rank)
        val_sampler = DistributedSampler(val_dataset, shuffle=False, num_replicas=world_size, rank=local_rank)
        test_sampler = DistributedSampler(test_dataset, shuffle=False, num_replicas=world_size, rank=local_rank)
    else:
        train_sampler = None
        val_sampler = None
        test_sampler = None
    
    # å…³é”®è°ƒè¯•ç‚¹ï¼šæ‰“å°å„è¿›ç¨‹æ•°æ®ç´¢å¼•èŒƒå›´
    if dist.is_initialized() and local_rank == 0:
        print(f"Rank {local_rank} è®­ç»ƒé›†ç´¢å¼•ç¤ºä¾‹: {list(train_sampler)[:5]}")
        print(f"Rank {local_rank} éªŒè¯é›†ç´¢å¼•ç¤ºä¾‹: {list(val_sampler)[:5]}")

    print("7")

    # é…ç½® DataLoader
    
    '''
    # åˆ›å»ºDataLoaderæ³¨æ„è¿™é‡Œç”¨batch_sampler
    train_loader = DataLoader(
        train_dataset,
        batch_sampler=train_sampler,  # ç”¨batch_sampleræ›¿ä»£sampler + batch_size
        num_workers=14,
        pin_memory=True,
        collate_fn=train_dataset.collate_fn,
        persistent_workers=True,
        prefetch_factor=2
    )
    val_loader = DataLoader(
        val_dataset,
        batch_sampler=val_sampler,
        num_workers=14,
        pin_memory=True,
        collate_fn=val_dataset.collate_fn,
        persistent_workers=True,
        prefetch_factor=2
    )
    test_loader = DataLoader(
        test_dataset,
        batch_sampler=test_sampler,
        num_workers=14,
        pin_memory=True,    
        collate_fn=test_dataset.collate_fn,
        persistent_workers=True,
        prefetch_factor=2
    )
    '''
    train_loader = train_dataset.create_dataloader(train_sampler, 512, 0)
    val_loader = val_dataset.create_dataloader( val_sampler,512, 0)
    test_loader = test_dataset.create_dataloader( test_sampler, 512, 0)
    
    # å¯åŠ¨è®­ç»ƒ
    trainer.train(train_loader, val_loader, test_loader, train_sampler=train_sampler, val_sampler=val_sampler)
    print("9")
